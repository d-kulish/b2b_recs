# TensorFlow Serving batching configuration
# Optimized for recommendation inference workloads

# Maximum batch size for inference
# Higher = more throughput, but higher latency for small batches
max_batch_size { value: 64 }

# Maximum time to wait for a batch to fill (microseconds)
# 10ms = good balance between latency and throughput
batch_timeout_micros { value: 10000 }

# Maximum number of batches waiting in queue
# Prevents memory issues under high load
max_enqueued_batches { value: 100 }

# Number of threads for processing batches
# Should match or be less than available vCPUs
num_batch_threads { value: 4 }

# Allow variable-length inputs (important for recommendation models)
pad_variable_length_inputs: true
