# Phase: ETL Domain

## Document Purpose
This document provides detailed specifications for the **ETL (Extract, Transform, Load)** domain in the ML Platform. The ETL domain manages data ingestion from external sources into BigQuery for model training.

**Last Updated**: 2026-02-03 (v16 - ETL Jobs: Added filter bar with Status, Connection, Schedule filters)

---

## Overview

### Purpose
The ETL domain allows users to:
1. Create and manage **Connections** to external data sources (databases, cloud storage, NoSQL)
2. Configure **ETL Jobs** that extract data from those connections into BigQuery
3. Schedule automated data extraction pipelines via Cloud Scheduler
4. Monitor ETL run history and troubleshoot failures

### Key Principles

1. **Connections are Reusable.** A single connection (e.g., "Production PostgreSQL") can be used by multiple ETL jobs. This prevents credential duplication and simplifies maintenance.

2. **Credentials are Secure.** Database passwords and API keys are stored in Google Secret Manager, not in the Django database. Only secret references are stored.

3. **ETL Jobs are Atomic.** Each ETL job extracts data from one connection to one BigQuery table. Complex pipelines are composed of multiple jobs.

4. **Scheduling is Optional.** Jobs can be manual-only or scheduled (hourly, daily, weekly, monthly) via Cloud Scheduler.

5. **Incremental Loads Supported.** Jobs can be configured for full replacement or incremental extraction based on a timestamp column.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ETL Domain                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Connections    â”‚ â”€â”€usesâ”€â”€â”‚    ETL Jobs      â”‚â”€â”€runsâ”€â”€â”€â”‚  Cloud Run   â”‚ â”‚
â”‚  â”‚   (Credentials)  â”‚         â”‚  (DataSource)    â”‚         â”‚  ETL Runner  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                            â”‚                          â”‚         â”‚
â”‚           â”‚                            â”‚                          â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Secret Manager  â”‚         â”‚ Cloud Scheduler  â”‚         â”‚   BigQuery   â”‚ â”‚
â”‚  â”‚  (Credentials)   â”‚         â”‚  (Automation)    â”‚         â”‚   (Target)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Stored

```
Connection (Django Model)
â”œâ”€â”€ Identification: name, description, source_type
â”œâ”€â”€ Database: host, port, database, schema, username
â”œâ”€â”€ Cloud Storage: bucket_path, service_account_json
â”œâ”€â”€ AWS: aws_access_key_id, aws_region
â”œâ”€â”€ Azure: azure_storage_account
â”œâ”€â”€ NoSQL: connection_string
â”œâ”€â”€ Security: credentials_secret_name (Secret Manager reference)
â”œâ”€â”€ Status: connection_tested, last_test_at, last_test_status
â””â”€â”€ Tracking: last_used_at, created_at, updated_at

DataSource (ETL Job - Django Model)
â”œâ”€â”€ Reference: connection (FK), etl_config (FK)
â”œâ”€â”€ Identification: name, source_type (denormalized)
â”œâ”€â”€ Schedule: schedule_type, cloud_scheduler_job_name
â”œâ”€â”€ Extraction: use_incremental, incremental_column, last_sync_value
â”œâ”€â”€ Status: is_enabled, last_run_at, last_run_status
â””â”€â”€ Tables: DataSourceTable[] (one-to-many)
```

---

## Chapter: Connections

### Purpose
Connections represent reusable database/storage credentials. They are created once and can be referenced by multiple ETL jobs.

### Supported Connection Types

#### Relational Databases
| Type | Key | Display Name | Authentication |
|------|-----|--------------|----------------|
| PostgreSQL | `postgresql` | PostgreSQL | Host, Port, Database, Username, Password |
| MySQL | `mysql` | MySQL | Host, Port, Database, Username, Password |
| MariaDB | `mariadb` | MariaDB | Host, Port, Database, Username, Password |
| Oracle | `oracle` | Oracle Database | Host, Port, Database, Username, Password |
| SQL Server | `sqlserver` | Microsoft SQL Server | Host, Port, Database, Username, Password |
| IBM DB2 | `db2` | IBM DB2 | Host, Port, Database, Username, Password |
| Amazon Redshift | `redshift` | Amazon Redshift | Host, Port, Database, Username, Password |
| Google BigQuery | `bigquery` | Google BigQuery | Project ID, Dataset, Service Account JSON |
| Snowflake | `snowflake` | Snowflake | Host, Port, Database, Username, Password |
| Azure Synapse | `synapse` | Azure Synapse | Host, Port, Database, Username, Password |
| Teradata | `teradata` | Teradata | Host, Port, Database, Username, Password |

#### Cloud Storage
| Type | Key | Display Name | Authentication |
|------|-----|--------------|----------------|
| Google Cloud Storage | `gcs` | Google Cloud Storage | Bucket Path (gs://), Service Account JSON |
| AWS S3 | `s3` | AWS S3 | Bucket Path (s3://), Access Key ID, Secret Access Key, Region |
| Azure Blob | `azure_blob` | Azure Blob Storage | Bucket Path, Storage Account, Account Key or SAS Token |

#### NoSQL Databases
| Type | Key | Display Name | Authentication |
|------|-----|--------------|----------------|
| MongoDB | `mongodb` | MongoDB | Connection String |
| Firestore | `firestore` | Google Firestore | Project ID, Service Account JSON |
| Cassandra | `cassandra` | Apache Cassandra | Connection String |
| DynamoDB | `dynamodb` | Amazon DynamoDB | AWS Credentials |
| Redis | `redis` | Redis | Connection String |

---

### User Interface

#### Connections List View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ Connections                                              [Test Connections] [+ Connection]     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STATUS          TYPE            [Search connections...                                        ]   â”‚
â”‚ [â–¼ All      ]   [â–¼ All      ]                                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚ â”‚ â— Production PostgreSQL â”‚  â”‚ â—‹ GCS Data Lake         â”‚  â”‚ â— BigQuery Analytics    â”‚             â”‚
â”‚ â”‚   Relational: analytics â”‚  â”‚   Cloud Storage: bucket â”‚  â”‚   Relational: project   â”‚             â”‚
â”‚ â”‚   Used by: 3 jobs       â”‚  â”‚   Used by: 1 job        â”‚  â”‚   Used by: 2 jobs       â”‚             â”‚
â”‚ â”‚   Tested: 2 min ago     â”‚  â”‚   Tested: Never         â”‚  â”‚   Tested: 5 min ago     â”‚             â”‚
â”‚ â”‚          [Edit] [Del]   â”‚  â”‚          [Edit] [Del]   â”‚  â”‚          [Edit] [Del]   â”‚             â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                                                    â”‚
â”‚ Showing 1-6 of 8 connections                              [< Previous] [1] [2] [Next >]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Filter Options:
- **Status**: All, Working, Failed, Not Tested
- **Type**: All, Relational, Cloud Storage, NoSQL

Status Indicators:
â— Green  = Connection tested successfully (within last hour)
â—‹ Gray   = Connection never tested or test status unknown
â— Red    = Connection test failed
```

#### Connection Card Structure

Cards are displayed in a responsive 3-column grid (2 columns on tablet, 1 on mobile).

Each connection card displays:
1. **Status Dot**: Visual indicator of connection health (green/gray/red)
2. **Connection Name**: User-defined friendly name
3. **Type Category**: "Relational", "Cloud Storage", or "NoSQL"
4. **Source Info**: Database name, bucket path, or connection string
5. **Usage Count**: Number of ETL jobs using this connection
6. **Last Tested**: Time since last connection test
7. **Actions**: Edit and Delete buttons

#### Create Connection Modal (2-Step Wizard)

**Step 1: Select Connection Type**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ Create Connection                                           Step 1 of 2 â”‚
â”‚    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚    [Type]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[Configure]                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ Select Connection Type                                                       â”‚
â”‚                                                                              â”‚
â”‚ [Relational DB] [Cloud Storage] [NoSQL DB]                                  â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ â—‹ PostgreSQL    â”‚ â”‚ â—‹ MySQL         â”‚ â”‚ â—‹ MariaDB       â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ â—‹ Oracle        â”‚ â”‚ â—‹ SQL Server    â”‚ â”‚ â—‹ IBM DB2       â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ â—‹ Redshift      â”‚ â”‚ â— BigQuery      â”‚ â”‚ â—‹ Snowflake     â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚ â”‚ â—‹ Azure Synapse â”‚ â”‚ â—‹ Teradata      â”‚                                    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    [Next â†’]      [Cancel]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 2: Configure Connection (varies by type)**

*Example: PostgreSQL Configuration*
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ Create Connection                                           Step 2 of 2 â”‚
â”‚    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚    [Type]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[Configure]                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ Configure PostgreSQL Connection                                              â”‚
â”‚                                                                              â”‚
â”‚ Connection Name *                                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Production PostgreSQL                                                   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Host *                         â”‚  â”‚ Port *                               â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚ â”‚ â”‚ db.example.com             â”‚ â”‚  â”‚ â”‚ 5432                             â”‚ â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚ Database *                                                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ analytics                                                               â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Username *                     â”‚  â”‚ Password *                           â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚ â”‚ â”‚ etl_user                   â”‚ â”‚  â”‚ â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                     â”‚ â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ âœ“ Connection tested successfully                                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                               [â† Back]  [Test Connection]  [Save] [Cancel]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Example: BigQuery Configuration*
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configure BigQuery Connection                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ Connection Name *                                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ BigQuery Analytics                                                      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Project ID *                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ my-gcp-project                                                          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Dataset *                                                                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ raw_data                                                                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Service Account JSON *                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ {                                                                       â”‚ â”‚
â”‚ â”‚   "type": "service_account",                                            â”‚ â”‚
â”‚ â”‚   "project_id": "my-gcp-project",                                       â”‚ â”‚
â”‚ â”‚   ...                                                                   â”‚ â”‚
â”‚ â”‚ }                                                                       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Example: GCS Configuration*
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configure Google Cloud Storage Connection                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ â„¹ï¸ Connection = access to bucket. File type selected during ETL job creation â”‚
â”‚                                                                              â”‚
â”‚ Connection Name *                                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ GCS Data Lake                                                           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Bucket Path *                                                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ gs://my-data-lake-bucket                                                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Service Account JSON *                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ { ... }                                                                 â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Data Model

#### Connection Model (`ml_platform/models.py`)

```python
class Connection(models.Model):
    # Foreign Keys
    model_endpoint = ForeignKey(ModelEndpoint)  # Parent model

    # Identification
    name = CharField(max_length=255)            # "Production PostgreSQL"
    source_type = CharField(max_length=50)      # "postgresql", "bigquery", etc.
    description = TextField(blank=True)

    # Database Connections
    source_host = CharField(max_length=255)     # "db.example.com"
    source_port = IntegerField(null=True)       # 5432
    source_database = CharField(max_length=255) # "analytics"
    source_schema = CharField(max_length=255)   # "public"
    source_username = CharField(max_length=255) # "etl_user"
    credentials_secret_name = CharField()       # Secret Manager reference

    # Cloud Storage
    bucket_path = CharField(max_length=512)     # "gs://bucket-name"

    # BigQuery/Firestore
    bigquery_project = CharField(max_length=255)
    bigquery_dataset = CharField(max_length=255)
    service_account_json = TextField()          # JSON key (encrypted in transit)

    # AWS S3
    aws_access_key_id = CharField(max_length=255)
    aws_secret_access_key_secret = CharField()  # Secret Manager reference
    aws_region = CharField(max_length=50)

    # Azure Blob
    azure_storage_account = CharField(max_length=255)
    azure_account_key_secret = CharField()      # Secret Manager reference
    azure_sas_token_secret = CharField()        # Alternative to account key

    # NoSQL
    connection_string = TextField()             # MongoDB, Redis, etc.
    connection_params = JSONField()             # Flexible additional params

    # Status
    is_enabled = BooleanField(default=True)
    connection_tested = BooleanField(default=False)
    last_test_at = DateTimeField(null=True)
    last_test_status = CharField(max_length=20) # "success" or "failed"
    last_test_message = TextField()

    # Usage
    last_used_at = DateTimeField(null=True)

    # Timestamps
    created_at = DateTimeField(auto_now_add=True)
    updated_at = DateTimeField(auto_now=True)

    class Meta:
        unique_together = [
            ['model_endpoint', 'name'],
            ['model_endpoint', 'source_type', 'source_host', 'source_port',
             'source_database', 'source_username']
        ]
```

---

### API Endpoints

#### Connection Management APIs

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/models/{id}/connections/test-wizard/` | Test connection and check for duplicates |
| `POST` | `/api/models/{id}/connections/create/` | Create new connection |
| `GET` | `/api/models/{id}/connections/` | List all connections for a model |
| `GET` | `/api/connections/{id}/` | Get connection details |
| `GET` | `/api/connections/{id}/credentials/` | Get decrypted credentials |
| `POST` | `/api/connections/{id}/test/` | Test existing connection |
| `POST` | `/api/connections/{id}/update/` | Update connection |
| `GET` | `/api/connections/{id}/usage/` | Get ETL jobs using this connection |
| `POST` | `/api/connections/{id}/delete/` | Delete connection |

#### Schema and Table Fetching APIs

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/connections/{id}/fetch-schemas/` | List available schemas/datasets |
| `POST` | `/api/connections/{id}/fetch-tables-for-schema/` | List tables in a schema |
| `POST` | `/api/connections/{id}/fetch-table-preview/` | Preview table data (10 rows) |

#### File Operations APIs (Cloud Storage)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/connections/{id}/list-files/` | List files in bucket/container |
| `POST` | `/api/connections/{id}/detect-file-schema/` | Auto-detect file schema (CSV, Parquet, JSON) |

---

### API Response Examples

#### List Connections Response
```json
{
  "status": "success",
  "connections": [
    {
      "id": 1,
      "name": "Production PostgreSQL",
      "source_type": "postgresql",
      "source_host": "db.example.com",
      "source_port": 5432,
      "source_database": "analytics",
      "connection_tested": true,
      "last_test_status": "success",
      "last_test_at": "2025-12-26T14:30:00Z",
      "jobs_count": 3
    },
    {
      "id": 2,
      "name": "GCS Data Lake",
      "source_type": "gcs",
      "bucket_path": "gs://data-lake-bucket",
      "connection_tested": false,
      "jobs_count": 1
    }
  ]
}
```

#### Test Connection Response (Success)
```json
{
  "status": "success",
  "message": "Connection successful. Found 15 tables.",
  "tables": [
    {"name": "customers", "row_count": 50000},
    {"name": "orders", "row_count": 1200000},
    {"name": "products", "row_count": 5000}
  ]
}
```

#### Test Connection Response (Duplicate Found)
```json
{
  "status": "success",
  "duplicate": true,
  "connection_id": 5,
  "connection_name": "Existing PostgreSQL Connection",
  "message": "Connection successful. This connection already exists."
}
```

#### Test Connection Response (Failure)
```json
{
  "status": "error",
  "message": "Connection failed: could not connect to server: Connection refused"
}
```

---

### JavaScript Functions

#### State Management
```javascript
// Global state
let allConnections = [];           // All connections from API
let connectionsCurrentPage = 1;     // Current pagination page
let connectionsSearchTerm = '';     // Current search filter

const ITEMS_PER_PAGE = 5;           // Items per page (configurable)
```

#### Core Functions

| Function | Purpose |
|----------|---------|
| `loadConnections()` | Fetch connections from API and populate `allConnections` |
| `renderConnectionsList()` | Render connection cards with pagination |
| `filterConnections(connections, term)` | Filter connections by search term |
| `handleConnectionsSearch()` | Handle search input with debounce |
| `goToConnectionsPage(page)` | Navigate to pagination page |
| `clearConnectionsSearch()` | Clear search and reset pagination |

#### Connection Testing

| Function | Purpose |
|----------|---------|
| `autoTestConnections(connections)` | Test all visible connections in background |
| `refreshConnections()` | Reload and re-test all connections |

#### CRUD Operations

| Function | Purpose |
|----------|---------|
| `openCreateConnectionModal()` | Open the create connection wizard |
| `closeCreateConnectionModal()` | Close the wizard modal |
| `openEditConnectionModal(id)` | Open edit modal for existing connection |
| `deleteConnection(id)` | Delete connection (with usage check) |
| `saveConnectionStandalone()` | Save new connection |

#### Wizard Navigation

| Function | Purpose |
|----------|---------|
| `goToConnStep(step)` | Navigate to wizard step (1 or 2) |
| `switchConnTab(tab)` | Switch between Relational/Storage/NoSQL tabs |
| `testAndProceed()` | Test connection and proceed to save |

---

### Connection Testing Logic

#### Test Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Clicks    â”‚ --> â”‚  API Call       â”‚ --> â”‚  Backend Test   â”‚
â”‚  "Test"         â”‚     â”‚  /test/         â”‚     â”‚  (socket + SQL) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                        â”‚  Update UI      â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  (status dot)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Backend Test Functions (`connection_manager.py`)

| Function | Source Type | What It Tests |
|----------|-------------|---------------|
| `test_postgresql()` | PostgreSQL | TCP connect + `SELECT 1` + list tables |
| `test_mysql()` | MySQL/MariaDB | TCP connect + `SELECT 1` + list tables |
| `test_bigquery()` | BigQuery | API auth + list tables in dataset |
| `test_firestore()` | Firestore | API auth + list collections |
| `test_gcs()` | GCS | List objects in bucket path |
| `test_s3()` | S3 | List objects in bucket path |
| `test_azure_blob()` | Azure Blob | List blobs in container |

#### Status Update Sequence

1. **On Page Load**: `autoTestConnections()` called for visible connections
2. **On Test Complete**: Status dot updated via DOM manipulation
3. **On Refresh Click**: All connections reloaded and retested

---

### Security Considerations

#### Credential Storage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Credential Flow                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. User enters credentials in form                                         â”‚
â”‚     â†“                                                                        â”‚
â”‚  2. Frontend sends to Django API (HTTPS)                                    â”‚
â”‚     â†“                                                                        â”‚
â”‚  3. Django stores in Google Secret Manager                                  â”‚
â”‚     Secret name: "etl-conn-{model_id}-{connection_id}"                      â”‚
â”‚     â†“                                                                        â”‚
â”‚  4. Django stores secret reference in Connection model                      â”‚
â”‚     credentials_secret_name = "etl-conn-5-12"                               â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸ Raw passwords NEVER stored in Django database                           â”‚
â”‚  âš ï¸ Service account JSON stored only in Secret Manager                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Duplicate Detection

When testing a new connection, the system checks for existing connections with:
- Same `source_type`
- Same `source_host`
- Same `source_port`
- Same `source_database`
- Same `source_username`

If found, user is prompted to use the existing connection instead.

---

### Error Handling

#### Connection Test Errors

| Error | Cause | User Message |
|-------|-------|--------------|
| Connection refused | Server down or firewall | "Connection refused. Check host/port." |
| Authentication failed | Wrong credentials | "Authentication failed. Check username/password." |
| Database not found | Wrong database name | "Database 'X' does not exist." |
| SSL required | Server requires SSL | "SSL connection required." |
| Timeout | Network issues | "Connection timed out. Check network." |
| Permission denied | Insufficient privileges | "Access denied. Check user permissions." |

#### Delete Protection

Connections cannot be deleted if:
- They are used by one or more ETL jobs

User sees: "Cannot delete connection: used by X ETL job(s). Delete or reassign those jobs first."

---

### Known Issues and Limitations

1. **No Connection Sharing Across Models**: Connections are scoped to a single ModelEndpoint. Cross-model sharing not yet implemented.

2. **No Connection Folders/Groups**: All connections displayed in flat list. For models with many connections, search and pagination help.

3. **Service Account JSON Stored in DB**: For BigQuery/GCS/Firestore, the service account JSON is currently stored in the model (encrypted). Migration to Secret Manager planned.

4. **No Connection Cloning**: Users cannot duplicate an existing connection. Must recreate manually.

---

### Future Enhancements

1. **Connection Templates**: Pre-configured connection templates for common setups
2. **Connection Health Dashboard**: Aggregate view of all connection statuses
3. **Scheduled Health Checks**: Automatic periodic testing of all connections
4. **Connection Import/Export**: Backup and restore connection configurations
5. **Role-Based Access**: Restrict connection management to admin users

---

## Chapter: ETL Jobs

### Purpose

ETL Jobs (also called DataSources) define data extraction pipelines that:
1. Extract data from a Connection (database, cloud storage, or NoSQL)
2. Load data into BigQuery tables
3. Run on-demand or on a schedule via Cloud Scheduler
4. Support both full replacement (Catalog) and incremental (Transactional) loads

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ETL Job Execution Flow                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    User      â”‚      â”‚   Cloud      â”‚      â”‚   Django     â”‚      â”‚  Cloud Run   â”‚ â”‚
â”‚  â”‚  (Manual)    â”‚      â”‚  Scheduler   â”‚      â”‚   Webhook    â”‚      â”‚    Job       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                     â”‚                     â”‚         â”‚
â”‚         â”‚  Click "Run"        â”‚  Cron trigger       â”‚                     â”‚         â”‚
â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚         â”‚
â”‚         â”‚                     â”‚                     â”‚                     â”‚         â”‚
â”‚         â”‚                     â”‚  POST /webhook      â”‚  run_v2.run_job()   â”‚         â”‚
â”‚         â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚         â”‚
â”‚         â”‚                     â”‚                     â”‚                     â”‚         â”‚
â”‚         â”‚                     â”‚                     â”‚   Create ETLRun     â”‚         â”‚
â”‚         â”‚                     â”‚                     â”‚   record (pending)  â”‚         â”‚
â”‚         â”‚                     â”‚                     â”‚                     â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚              â”‚ â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚  ETL Runner  â”‚ â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚  (main.py)   â”‚ â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚      â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                           â”‚         â”‚
â”‚                                                                           â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         Processing Mode Decision                              â”‚  â”‚
â”‚  â”‚                                                                               â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚    â”‚  Estimated Rows     â”‚              â”‚  Processing Mode    â”‚              â”‚  â”‚
â”‚  â”‚    â”‚  < 1,000,000        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  STANDARD           â”‚              â”‚  â”‚
â”‚  â”‚    â”‚                     â”‚              â”‚  (Pandas + Cloud    â”‚              â”‚  â”‚
â”‚  â”‚    â”‚                     â”‚              â”‚   Run Job)          â”‚              â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                               â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚    â”‚  Estimated Rows     â”‚              â”‚  Processing Mode    â”‚              â”‚  â”‚
â”‚  â”‚    â”‚  >= 1,000,000       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  DATAFLOW           â”‚              â”‚  â”‚
â”‚  â”‚    â”‚                     â”‚              â”‚  (Apache Beam +     â”‚              â”‚  â”‚
â”‚  â”‚    â”‚                     â”‚              â”‚   Dataflow)         â”‚              â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Processing Modes

#### Standard Processing (Cloud Run Job)

For datasets **< 1 million rows**, uses:
- **Pandas** for data extraction and transformation
- **Cloud Run Job** (`etl-runner`) for execution
- **Batch loading** to BigQuery (10,000 rows per batch)

**Advantages:**
- Faster startup (no Dataflow worker spin-up)
- Lower cost for small datasets
- Simpler debugging

**Flow:**
```
Source DB/Storage â”€â”€â–¶ Pandas DataFrame â”€â”€â–¶ BigQuery Load Job â”€â”€â–¶ BigQuery Table
                     (in Cloud Run)        (batch upload)
```

#### Dataflow Processing (Apache Beam)

For datasets **>= 1 million rows**, uses:
- **Apache Beam** pipeline for distributed processing
- **Dataflow** workers for parallel execution
- **Native BigQuery I/O** for efficient loading

**Advantages:**
- Horizontal scaling for large datasets
- Parallel processing across multiple workers
- Handles datasets of any size

**Flow:**
```
Source DB/Storage â”€â”€â–¶ Apache Beam â”€â”€â–¶ Dataflow Workers â”€â”€â–¶ BigQuery (streaming)
                     (pipeline)      (auto-scaled)
```

#### Processing Mode Selection

| Mode | Setting | Behavior |
|------|---------|----------|
| Auto (default) | `processing_mode='auto'` | Estimate rows, use Dataflow if >= threshold |
| Standard | `processing_mode='standard'` | Always use Pandas + Cloud Run |
| Dataflow | `processing_mode='dataflow'` | Always use Apache Beam + Dataflow |

**Threshold:** Default 1,000,000 rows (configurable per table via `row_count_threshold`)

---

### Load Types

#### Catalog (Full Snapshot)

- **Replaces** all data in destination table
- Used for dimension/reference data that changes infrequently
- Example: Product catalog, customer master data

```
Source Table                    BigQuery Table
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ products       â”‚    REPLACE   â”‚ products       â”‚
â”‚ - id           â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ - id           â”‚
â”‚ - name         â”‚    ALL       â”‚ - name         â”‚
â”‚ - price        â”‚              â”‚ - price        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Transactional (Incremental/Append)

- **Appends** only new/changed records since last sync
- Uses timestamp column to track changes
- Tracks `last_sync_value` for next run

```
Source Table                    BigQuery Table
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ orders         â”‚    APPEND    â”‚ orders         â”‚
â”‚ - id           â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ - id           â”‚
â”‚ - created_at   â”‚    WHERE     â”‚ - created_at   â”‚
â”‚ - amount       â”‚ created_at > â”‚ - amount       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ last_sync    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Data Model

#### DataSource (ETL Job)

```python
class DataSource(models.Model):
    # Parent references
    etl_config = ForeignKey(ETLConfiguration)
    connection = ForeignKey(Connection)       # Reusable connection

    # Identification
    name = CharField(max_length=255)          # "Daily Transactions Extract"
    source_type = CharField(max_length=50)    # Denormalized from connection

    # Status
    is_enabled = BooleanField(default=True)

    # Schedule
    schedule_type = CharField()               # 'manual', 'hourly', 'daily', 'weekly', 'monthly'
    cloud_scheduler_job_name = CharField()    # Full path to Cloud Scheduler job

    # Extraction settings
    use_incremental = BooleanField()
    incremental_column = CharField()
    last_sync_value = CharField()
    historical_start_date = DateField()

    # Last run tracking
    last_run_at = DateTimeField()
    last_run_status = CharField()             # 'completed', 'failed', 'running'
    last_run_message = TextField()

    # Timestamps
    created_at = DateTimeField()
    updated_at = DateTimeField()
```

#### DataSourceTable (Table Configuration)

```python
class DataSourceTable(models.Model):
    data_source = ForeignKey(DataSource)

    # Source configuration
    schema_name = CharField()                 # "public"
    source_table_name = CharField()           # "transactions"
    source_query = TextField()                # Custom SQL (optional)

    # Destination configuration
    dest_table_name = CharField()             # "transactions"
    dest_dataset = CharField()                # "raw_data"

    # Load strategy
    load_type = CharField()                   # 'transactional' or 'catalog'
    timestamp_column = CharField()            # For incremental loads
    historical_start_date = DateField()

    # Column selection
    selected_columns = JSONField()            # [] = all columns

    # File source configuration (GCS/S3/Azure)
    is_file_based = BooleanField()
    file_path_prefix = CharField()            # "data/transactions/"
    file_pattern = CharField()                # "*.csv"
    file_format = CharField()                 # 'csv', 'parquet', 'json'
    file_format_options = JSONField()         # delimiter, encoding, etc.
    column_mapping = JSONField()              # Original -> sanitized names

    # Processing configuration
    processing_mode = CharField()             # 'auto', 'standard', 'dataflow'
    row_count_threshold = IntegerField()      # Default: 1,000,000
    estimated_row_count = BigIntegerField()   # From last run

    # Schedule (per-table)
    schedule_type = CharField()
    schedule_time = TimeField()
    schedule_minute = IntegerField()          # For hourly
    schedule_day_of_week = IntegerField()     # 0-6 for weekly
    schedule_day_of_month = IntegerField()    # 1-31 for monthly
    schedule_timezone = CharField()           # Default: 'UTC'

    # Statistics
    last_row_count = IntegerField()
    last_synced_at = DateTimeField()
```

#### ETLRun (Execution History)

```python
class ETLRun(models.Model):
    # References
    etl_config = ForeignKey(ETLConfiguration)
    model_endpoint = ForeignKey(ModelEndpoint)
    data_source = ForeignKey(DataSource)

    # Status
    status = CharField()                      # 'pending', 'running', 'completed', 'failed'
    started_at = DateTimeField()
    completed_at = DateTimeField()

    # Cloud Run execution
    cloud_run_execution_id = CharField()

    # Progress tracking
    extraction_started_at = DateTimeField()
    extraction_completed_at = DateTimeField()
    loading_started_at = DateTimeField()
    loading_completed_at = DateTimeField()

    # Results
    total_sources = IntegerField()
    successful_sources = IntegerField()
    total_tables = IntegerField()
    successful_tables = IntegerField()
    total_rows_extracted = BigIntegerField()
    rows_loaded = BigIntegerField()
    bytes_processed = BigIntegerField()
    duration_seconds = IntegerField()

    # Details
    results_detail = JSONField()              # Per-table results
    error_message = TextField()
    logs_url = URLField()                     # Cloud Run logs

    triggered_by = ForeignKey(User)           # Null for scheduled runs
    created_at = DateTimeField()
```

#### ProcessedFile (File Tracking)

```python
class ProcessedFile(models.Model):
    """Tracks processed files for incremental file-based ETL"""
    data_source_table = ForeignKey(DataSourceTable)

    file_path = CharField()                   # Full path in storage
    file_size_bytes = BigIntegerField()
    file_last_modified = DateTimeField()
    rows_loaded = IntegerField()
    processed_at = DateTimeField()
    etl_run = ForeignKey(ETLRun)
```

---

### Cloud Services Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              GCP Services                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Cloud Scheduler                                                              â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚ â”‚ etl-job-5       â”‚  â”‚ etl-job-7       â”‚  â”‚ etl-job-14      â”‚              â”‚    â”‚
â”‚  â”‚ â”‚ Daily 09:00 UTC â”‚  â”‚ Weekly Mon 08:00â”‚  â”‚ Hourly :30      â”‚              â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â”‚                    â”‚                    â”‚                              â”‚
â”‚             â”‚  HTTP POST with OIDC token              â”‚                              â”‚
â”‚             â–¼                    â–¼                    â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Cloud Run Service (Django App)                                               â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ Webhook Endpoint: /api/etl/sources/{id}/scheduler-webhook/            â”‚   â”‚    â”‚
â”‚  â”‚ â”‚                                                                        â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 1. Validate OIDC token                                                â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 2. Create ETLRun record (status='pending')                            â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 3. Trigger Cloud Run Job via run_v2.run_job()                         â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                         â”‚                                            â”‚
â”‚                                         â”‚ run_v2.run_job()                           â”‚
â”‚                                         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Cloud Run Job: etl-runner                                                    â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ Entry Point: python main.py --data_source_id=X --etl_run_id=Y         â”‚   â”‚    â”‚
â”‚  â”‚ â”‚                                                                        â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 1. Fetch job config from Django API                                   â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 2. Determine processing mode (standard vs dataflow)                   â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 3. Execute extraction + loading                                       â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ 4. Update ETLRun status via Django API                                â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                         â”‚                                            â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚             â”‚                           â”‚                           â”‚                â”‚
â”‚             â–¼                           â–¼                           â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Source Database     â”‚  â”‚ Cloud Storage       â”‚  â”‚ BigQuery            â”‚          â”‚
â”‚  â”‚ (PostgreSQL, MySQL, â”‚  â”‚ (GCS, S3, Azure)    â”‚  â”‚ (Destination)       â”‚          â”‚
â”‚  â”‚  BigQuery, etc.)    â”‚  â”‚                     â”‚  â”‚                     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                                      â”‚
â”‚                              FOR LARGE DATASETS (>= 1M rows):                        â”‚
â”‚                                         â”‚                                            â”‚
â”‚                                         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Dataflow                                                                     â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚ â”‚ Apache Beam Pipeline                                                   â”‚   â”‚    â”‚
â”‚  â”‚ â”‚                                                                        â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ Source â”€â”€â–¶ Transform â”€â”€â–¶ Partition â”€â”€â–¶ Load â”€â”€â–¶ BigQuery              â”‚   â”‚    â”‚
â”‚  â”‚ â”‚                                                                        â”‚   â”‚    â”‚
â”‚  â”‚ â”‚ Auto-scaled workers (n1-standard-2)                                   â”‚   â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Cloud Scheduler Integration

#### Job Naming Convention

```
projects/{project_id}/locations/{region}/jobs/etl-job-{data_source_id}

Example:
projects/b2b-recs/locations/europe-central2/jobs/etl-job-14
```

#### Schedule Types and Cron Expressions

| Schedule Type | Example Config | Cron Expression |
|---------------|----------------|-----------------|
| Hourly | minute=30 | `30 * * * *` |
| Daily | time=09:00 | `0 9 * * *` |
| Weekly | day=1 (Mon), time=08:00 | `0 8 * * 1` |
| Monthly | day=15, time=06:00 | `0 6 15 * *` |

#### Webhook Payload

Cloud Scheduler sends:
```json
{
  "data_source_id": 14,
  "trigger": "scheduled"
}
```

#### OIDC Authentication

- Cloud Scheduler uses **OIDC token** for authentication
- Service account: `{project-number}-compute@developer.gserviceaccount.com`
- Audience: Django Cloud Run service URL

---

### ETL Runner Microservice

#### Directory Structure

```
etl_runner/
â”œâ”€â”€ main.py                    # Entry point, ETLRunner class
â”œâ”€â”€ config.py                  # Configuration management, Django API client
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ base.py               # Base extractor interface
â”‚   â”œâ”€â”€ postgresql.py         # PostgreSQL extraction
â”‚   â”œâ”€â”€ mysql.py              # MySQL extraction
â”‚   â”œâ”€â”€ bigquery.py           # BigQuery extraction
â”‚   â”œâ”€â”€ firestore.py          # Firestore extraction
â”‚   â””â”€â”€ file_extractor.py     # GCS/S3/Azure file extraction
â”œâ”€â”€ loaders/
â”‚   â””â”€â”€ bigquery_loader.py    # BigQuery loading (batch + streaming)
â”œâ”€â”€ dataflow_pipelines/
â”‚   â”œâ”€â”€ etl_pipeline.py       # Apache Beam pipeline definitions
â”‚   â””â”€â”€ partitioning.py       # Work unit calculation for parallel processing
â””â”€â”€ utils/
    â”œâ”€â”€ logging_config.py     # Structured JSON logging for Cloud Run
    â””â”€â”€ error_handling.py     # Error handling and status updates
```

#### Entry Point Arguments

```bash
python main.py \
  --data_source_id=14 \
  --etl_run_id=567 \
  --log_level=INFO \
  --json_logs
```

#### Environment Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `DJANGO_API_URL` | Django Cloud Run URL | `https://django-app-xxx.run.app` |
| `ETL_API_TOKEN` | API authentication token | `secret-token` |
| `GCP_PROJECT_ID` | GCP project | `b2b-recs` |
| `BIGQUERY_DATASET` | Default destination dataset | `raw_data` |
| `DATAFLOW_BUCKET` | GCS bucket for Dataflow temp files | `b2b-recs-dataflow` |
| `DATAFLOW_REGION` | Dataflow region | `europe-central2` |
| `ETL_BATCH_SIZE` | Rows per batch | `10000` |

---

### API Endpoints

#### ETL Job Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/models/{id}/etl/create-job/` | Create new ETL job |
| `GET` | `/api/etl/sources/{id}/` | Get ETL job details |
| `POST` | `/api/etl/sources/{id}/edit/` | Update ETL job |
| `POST` | `/api/etl/sources/{id}/delete/` | Delete ETL job |
| `POST` | `/api/etl/sources/{id}/toggle-pause/` | Pause/resume scheduler |

#### ETL Execution

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/etl/sources/{id}/run/` | Trigger manual run |
| `POST` | `/api/etl/sources/{id}/trigger/` | Trigger via API (internal) |
| `POST` | `/api/etl/sources/{id}/scheduler-webhook/` | Cloud Scheduler webhook |

#### ETL Runner APIs (Internal)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/etl/job-config/{id}/` | Get job configuration for runner |
| `PATCH` | `/api/etl/runs/{id}/update/` | Update run status/progress |
| `GET` | `/api/etl/sources/{id}/processed-files/` | Get processed files list |
| `POST` | `/api/etl/sources/{id}/record-processed-file/` | Record file as processed |

#### ETL Run Status

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/etl/runs/{id}/status/` | Get run status |

---

### User Interface

#### ETL Jobs List View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETL Jobs                                                [+ New ETL Job]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ” [Search ETL jobs...                                                  ]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â— Daily Transactions                                                    â”‚ â”‚
â”‚ â”‚   From: public.transactions                                             â”‚ â”‚
â”‚ â”‚   To: transactions                                                      â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚   Schedule: Daily at 09:00        Last run: 2 hours ago                â”‚ â”‚
â”‚ â”‚   Connection: Prod PostgreSQL     Status: Success                       â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚   [â–¶ Run] [â¸ Pause]              [Edit] [Delete]                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â—‹ Weekly Products Sync (PAUSED)                                         â”‚ â”‚
â”‚ â”‚   From: inventory.products                                              â”‚ â”‚
â”‚ â”‚   To: products                                                          â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚   Schedule: Weekly Mon 08:00      Last run: 5 days ago                 â”‚ â”‚
â”‚ â”‚   Connection: GCS Data Lake       Status: Success                       â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚   [â–¶ Run] [â–¶ Resume]             [Edit] [Delete]                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚ Showing 1-2 of 8 jobs                                                       â”‚
â”‚                                      [< Previous] [1] [2] [Next >]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Status Indicators:
â— Green  = Enabled and running on schedule
â—‹ Gray   = Paused or manual-only
â— Red    = Last run failed
```

#### ETL Job Card Structure

Each job card displays 5 columns:
1. **Job Info**: Name, source â†’ destination table mapping
2. **Schedule + Connection**: Schedule type/time, connection name
3. **Last Run Info**: When last ran, status
4. **Run Actions**: Run Now button, Pause/Resume button (if scheduled)
5. **Actions**: Edit and Delete buttons

---

### File Change Detection

For file-based sources (GCS, S3, Azure Blob), the ETL runner detects changes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         File Change Detection Flow                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. Fetch previously processed files from ProcessedFile table               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ ProcessedFile                                                     â”‚    â”‚
â”‚     â”‚ - file_path: gs://bucket/data/file1.csv                          â”‚    â”‚
â”‚     â”‚ - file_size_bytes: 1024000                                       â”‚    â”‚
â”‚     â”‚ - file_last_modified: 2025-12-25T10:00:00Z                       â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚  2. List current files in cloud storage                                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ gs://bucket/data/                                                 â”‚    â”‚
â”‚     â”‚ - file1.csv (unchanged)                                          â”‚    â”‚
â”‚     â”‚ - file2.csv (NEW)                                                â”‚    â”‚
â”‚     â”‚ - file3.csv (MODIFIED - size changed)                            â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚  3. Compare and determine action:                                           â”‚
â”‚                                                                              â”‚
â”‚     CATALOG mode: Any change detected â†’ process ALL files                   â”‚
â”‚     TRANSACTIONAL mode: Process only NEW/MODIFIED files                     â”‚
â”‚                                                                              â”‚
â”‚  4. After processing, record files in ProcessedFile table                   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Error Handling

#### Error Types

| Error Class | When Raised | Handling |
|-------------|-------------|----------|
| `ConfigurationError` | Invalid job config | Fail immediately, update status |
| `ExtractionError` | Source connection/query fails | Fail, log error details |
| `LoadError` | BigQuery loading fails | Fail, partial data may be loaded |

#### Status Updates

The ETL runner updates Django via API at:
1. **Start**: `status='running'`
2. **Progress**: Every 5 batches (rows extracted/loaded)
3. **Complete**: `status='completed'`, final row counts, duration
4. **Failure**: `status='failed'`, error message

#### Retry Logic

- Default: 3 retries with 5-second delay
- Applies to: Database connections, API calls
- Does NOT retry: Configuration errors, authentication failures

---

### Known Issues and Limitations

1. **No Job Dependencies**: Jobs run independently, cannot be chained
2. **Single Table per Run**: Each job processes one source â†’ one destination
3. **No Data Validation**: Schema compatibility checked at creation, not runtime
4. **Dataflow Cold Start**: First Dataflow job in a session takes 2-3 minutes to start workers

---

### Future Enhancements

1. **Job Templates**: Pre-configured job templates for common patterns
2. **Job Chaining**: Run jobs in sequence (DAG-style)
3. **Data Quality Checks**: Row count validation, schema drift detection
4. **Alerting**: Email/Slack notifications on failure
5. **Cost Estimation**: Estimate Dataflow cost before running

---

## Chapter: ETL Jobs Dashboard

The ETL Jobs Dashboard provides operational visibility into ETL pipeline performance through summary KPIs and execution history.

### KPI Cards

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         ETL Jobs Dashboard                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â–¶ 102        â”‚  â”‚ âœ“ 89.2%     â”‚  â”‚ âœ“ 91         â”‚  â”‚ âœ— 8          â”‚  â”‚ â‰¡ 164,101    â”‚  â”‚ â± 37s    â”‚ â”‚
â”‚  â”‚   TOTAL RUNS â”‚  â”‚   SUCCESS   â”‚  â”‚   SUCCESSFUL â”‚  â”‚   FAILED     â”‚  â”‚   ROWS       â”‚  â”‚   AVG    â”‚ â”‚
â”‚  â”‚              â”‚  â”‚   RATE      â”‚  â”‚   RUNS       â”‚  â”‚   RUNS       â”‚  â”‚   MIGRATED   â”‚  â”‚   DURATIONâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       (blue)          (green)           (green)           (red)           (purple)         (blue)       â”‚
â”‚                                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### KPI Definitions

| KPI | Icon | Color | Description | Data Source |
|-----|------|-------|-------------|-------------|
| **Total Runs** | â–¶ Play | Blue | Total number of ETL runs executed across all jobs | `COUNT(ETLRun)` |
| **Success Rate** | âœ“ Check | Green | Percentage of runs that completed successfully | `(successful / total) Ã— 100` |
| **Successful Runs** | âœ“ Check | Green | Count of runs with `status='completed'` | `COUNT(ETLRun WHERE status='completed')` |
| **Failed Runs** | âœ— Cross | Red | Count of runs with `status='failed'` | `COUNT(ETLRun WHERE status='failed')` |
| **Rows Migrated** | â‰¡ Database | Purple | Total rows loaded across all successful runs | `SUM(ETLRun.rows_loaded)` |
| **Avg Duration** | â± Clock | Blue | Average execution time of completed runs | `AVG(ETLRun.duration_seconds)` |

#### KPI Details

##### Total Runs
- **Purpose**: Provides overall volume indicator for ETL activity
- **Calculation**: Simple count of all `ETLRun` records for the current model/ETL configuration
- **Includes**: All runs regardless of status (pending, running, completed, failed)
- **Use Case**: Monitor ETL activity levels, detect unusual patterns (sudden drop or spike)

##### Success Rate
- **Purpose**: Primary health indicator for ETL pipelines
- **Calculation**: `(Successful Runs / Total Runs) Ã— 100`
- **Format**: Displayed as percentage with one decimal place (e.g., "89.2%")
- **Thresholds**:
  - `>= 95%`: Healthy (green)
  - `80-95%`: Warning (yellow)
  - `< 80%`: Critical (red)
- **Use Case**: Quick assessment of pipeline reliability, SLA monitoring

##### Successful Runs
- **Purpose**: Absolute count of successful ETL executions
- **Criteria**: `ETLRun.status = 'completed'`
- **Use Case**: Paired with Total Runs to understand raw success volume

##### Failed Runs
- **Purpose**: Track failures requiring attention
- **Criteria**: `ETLRun.status = 'failed'`
- **Action**: Click to filter run history to failed runs only
- **Use Case**: Identify jobs needing investigation, track failure trends

##### Rows Migrated
- **Purpose**: Measure data throughput volume
- **Calculation**: `SUM(rows_loaded)` from all completed runs
- **Format**: Formatted with thousands separator (e.g., "164,101")
- **Use Case**: Capacity planning, verify data completeness, billing estimates

##### Avg Duration
- **Purpose**: Performance baseline for ETL jobs
- **Calculation**: `AVG(duration_seconds)` from completed runs
- **Format**: Displayed in seconds (e.g., "37s") or minutes for longer durations
- **Use Case**: Performance monitoring, detect degradation over time

---

### Data Aggregation

#### Time Scope

KPIs are calculated across **all historical runs** for the current ETL configuration. Future enhancement may add time-based filtering (last 7 days, last 30 days, etc.).

#### Query Logic

```python
# Backend calculation (ml_platform/etl/api.py)
def get_dashboard_stats(etl_config_id):
    runs = ETLRun.objects.filter(etl_config_id=etl_config_id)

    total_runs = runs.count()
    successful_runs = runs.filter(status='completed').count()
    failed_runs = runs.filter(status='failed').count()

    success_rate = (successful_runs / total_runs * 100) if total_runs > 0 else 0

    completed_runs = runs.filter(status='completed')
    rows_migrated = completed_runs.aggregate(Sum('rows_loaded'))['rows_loaded__sum'] or 0
    avg_duration = completed_runs.aggregate(Avg('duration_seconds'))['duration_seconds__avg'] or 0

    return {
        'total_runs': total_runs,
        'success_rate': round(success_rate, 1),
        'successful_runs': successful_runs,
        'failed_runs': failed_runs,
        'rows_migrated': rows_migrated,
        'avg_duration': int(avg_duration)
    }
```

---

### UI Implementation

#### Card Structure

Each KPI card follows a consistent structure:

```html
<div class="kpi-card">
    <div class="kpi-icon kpi-icon--{color}">
        <i class="fas fa-{icon}"></i>
    </div>
    <div class="kpi-content">
        <div class="kpi-value">{value}</div>
        <div class="kpi-label">{label}</div>
    </div>
</div>
```

#### Icon Mapping

| KPI | Font Awesome Icon | CSS Class |
|-----|-------------------|-----------|
| Total Runs | `fa-play` | `kpi-icon--blue` |
| Success Rate | `fa-check-circle` | `kpi-icon--green` |
| Successful Runs | `fa-check` | `kpi-icon--green` |
| Failed Runs | `fa-times-circle` | `kpi-icon--red` |
| Rows Migrated | `fa-database` | `kpi-icon--purple` |
| Avg Duration | `fa-clock` | `kpi-icon--blue` |

#### Responsive Behavior

- **Desktop (>1200px)**: 6 cards in single row
- **Tablet (768-1200px)**: 3 cards per row (2 rows)
- **Mobile (<768px)**: 2 cards per row (3 rows)

---

### Scheduled Jobs Table

The Scheduled Jobs table displays all ETL jobs configured with automated schedules, showing their next run time and current state.

#### Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         Dashboard Row 2                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                               â”‚                                                           â”‚
â”‚  ğŸ“… Scheduled Jobs                            â”‚  ETL Job Runs (Last 5 Days)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                                           â”‚
â”‚  â”‚ Job Name    â”‚ Schedule  â”‚ Next Run    â”‚   â”‚              [Bubble Chart]                              â”‚
â”‚  â”‚             â”‚           â”‚             â”‚   â”‚                                                           â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚                                                           â”‚
â”‚  â”‚ Daily Trans â”‚ Daily 09:00â”‚ Dec 27, 09:00â”‚  â”‚                                                           â”‚
â”‚  â”‚ Weekly Prod â”‚ Mon 08:00 â”‚ Dec 30, 08:00â”‚  â”‚                                                           â”‚
â”‚  â”‚ Hourly Inv  â”‚ Hourly :30â”‚ Dec 26, 15:30â”‚  â”‚                                                           â”‚
â”‚  â”‚ Monthly Rep â”‚ 1st 06:00 â”‚ Jan 1, 06:00 â”‚  â”‚                                                           â”‚
â”‚  â”‚ Old Job     â”‚ Daily 02:00â”‚ â€” (Paused) â”‚   â”‚                                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                                           â”‚
â”‚                                               â”‚                                                           â”‚
â”‚  1-5 of 8    [Prev] [1] [2] [Next]           â”‚                                                           â”‚
â”‚                                               â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Table Columns

| Column | Description | Data Source |
|--------|-------------|-------------|
| **Job Name** | ETL job name (truncated to 18 chars with tooltip) | `DataSource.name` |
| **Schedule** | Human-readable schedule (e.g., "Daily 09:00", "Mon 08:00") | Derived from schedule fields |
| **Next Run** | Next scheduled execution time in job's timezone | Cloud Scheduler API |
| **State** | Current scheduler state badge | Cloud Scheduler API |

#### State Badges

| State | Badge | Description |
|-------|-------|-------------|
| `ENABLED` | `âœ“ Enabled` (green) | Job is active and will run on schedule |
| `PAUSED` | `â¸ Paused` (gray) | Job is paused, next run shows "â€”" |
| `UNKNOWN` | Based on `is_enabled` | Could not fetch status from Cloud Scheduler |

#### Filtering and Sorting

Jobs are displayed in the following order:
1. **Enabled jobs first**: Sorted by `next_run_time` (soonest first)
2. **Paused jobs last**: Sorted alphabetically by name

#### Data Aggregation

```python
# Backend: ml_platform/etl/views.py

# 1. Get all scheduled data sources (non-manual with scheduler job)
scheduled_sources = data_sources.filter(
    schedule_type__in=['hourly', 'daily', 'weekly', 'monthly'],
    cloud_scheduler_job_name__isnull=False
).exclude(cloud_scheduler_job_name='')

# 2. For each source, fetch status from Cloud Scheduler API
for source in scheduled_sources:
    status = scheduler_manager.get_schedule_status(source.cloud_scheduler_job_name)
    next_run_time = status.get('next_run_time')
    state = status.get('state')  # 'ENABLED', 'PAUSED', etc.
    is_paused = (state == 'PAUSED')

# 3. Sort: enabled by next_run_time, paused alphabetically
enabled_jobs.sort(key=lambda x: x['next_run_time'])
paused_jobs.sort(key=lambda x: x['name'].lower())
scheduled_jobs_list = enabled_jobs + paused_jobs
```

#### Schedule Display Format

| Schedule Type | Display Format | Example |
|---------------|----------------|---------|
| Hourly | `Hourly :MM` | "Hourly :30" |
| Daily | `Daily HH:MM` | "Daily 09:00" |
| Weekly | `DAY HH:MM` | "Mon 08:00" |
| Monthly | `Nth HH:MM` | "15th 06:00" |

#### Pagination

- **Items per page**: 5
- **URL parameter**: `sched_page`
- **Shows**: "1-5 of 8" format with Previous/Next navigation

#### Empty State

When no scheduled jobs exist:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ğŸ“…                              â”‚
â”‚      No scheduled jobs              â”‚
â”‚      Create ETL jobs with           â”‚
â”‚      schedules to see them here     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ETL Job Runs Bubble Chart

The bubble chart provides a visual timeline of ETL job executions over the last 5 days, with bubble attributes encoding run metadata.

#### Chart Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETL Job Runs (Last 5 Days)                    â— Success  â— Partial  â— Failed  â”‚ â— Data  â—‹ No data      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                                          â”‚
â”‚                       Dec 22        Dec 23        Dec 24        Dec 25        Dec 26                    â”‚
â”‚                          â”‚             â”‚             â”‚             â”‚             â”‚                       â”‚
â”‚  Daily Transactions â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€                  â”‚
â”‚                         (lg)          (lg)          (lg)          (lg)          (md)                    â”‚
â”‚                                                                                                          â”‚
â”‚  Weekly Products â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚                                                                     (sm)                                 â”‚
â”‚                                                                                                          â”‚
â”‚  Hourly Inventory â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—                  â”‚
â”‚                         (xs) ...                              (xs) ...                                   â”‚
â”‚                                                                                                          â”‚
â”‚  Monthly Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚                                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
  â— Filled bubble = Rows were loaded (data transferred)
  â—‹ Hollow bubble = No data loaded (0 rows)
  Bubble size = Duration (larger = longer running)
  Color: Green = Success, Orange = Partial, Red = Failed
```

#### Visual Encoding

| Attribute | Encoding | Description |
|-----------|----------|-------------|
| **X Position** | Time | When the run started (5-day range) |
| **Y Position** | Job Name | Which ETL job was executed |
| **Bubble Size** | Duration | Execution time in seconds (scaled minâ†’max to 4pxâ†’14px radius) |
| **Bubble Color** | Status | `completed`=green, `partial`=orange, `failed`=red |
| **Bubble Fill** | Data Loaded | Filled=rows loaded, Hollow=no data (0 rows) |

#### Status Color Mapping

| Status | Color | Hex | Condition |
|--------|-------|-----|-----------|
| Success | Green | `#22C55E` | `status='completed'` |
| Partial | Orange | `#FB923C` | `status='completed'` but partial success |
| Failed | Red | `#EF4444` | `status='failed'` |

#### Bubble Size Scale

```javascript
// Size is scaled based on duration relative to all runs in the 5-day window
const minRadius = 4;   // Minimum bubble radius (px)
const maxRadius = 14;  // Maximum bubble radius (px)

// Linear scale from min to max duration
const sizeScale = d3.scaleLinear()
    .domain([durationStats.min, durationStats.max])
    .range([minRadius, maxRadius])
    .clamp(true);
```

#### Data Structure

```javascript
// bubble_chart_data passed from Django to JavaScript
{
    "runs": [
        {
            "job_name": "Daily Transactions",
            "started_at": "2025-12-26T09:00:00+00:00",
            "duration": 37,        // seconds
            "status": "completed", // or "failed", "partial"
            "rows_loaded": 15420
        },
        // ... more runs
    ],
    "job_names": ["Daily Transactions", "Hourly Inventory", "Weekly Products"],
    "date_range": {
        "start": "2025-12-22T00:00:00+00:00",
        "end": "2025-12-26T23:59:59+00:00"
    },
    "duration_stats": {
        "min": 12,   // shortest run in seconds
        "max": 145   // longest run in seconds
    }
}
```

#### Tooltip

On hover, each bubble displays a tooltip with:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Daily Transactions             â”‚
â”‚ Dec 26, 09:00                  â”‚
â”‚ Duration: 37s                  â”‚
â”‚ Rows: 15,420                   â”‚
â”‚ Status: âœ“ Completed            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Chart Dimensions

| Property | Value | Notes |
|----------|-------|-------|
| Container width | 100% | Responsive to parent |
| Chart height | 260px | Fixed for consistent layout |
| Margins | `{ top: 15, right: 30, bottom: 40, left: 130 }` | Left margin for job names |
| Y-axis | Categorical (job names) | Uses `d3.scaleBand()` |
| X-axis | Time scale (5 days) | Uses `d3.scaleTime()` |

#### Rendering Library

Uses **D3.js** for SVG rendering with the following components:
- `d3.scaleTime()` for X-axis
- `d3.scaleBand()` for Y-axis (job names)
- `d3.scaleLinear()` for bubble size
- Custom tooltip positioning

#### Empty State

When no runs exist in the last 5 days:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸ“Š                         â”‚
â”‚      No job runs                    â”‚
â”‚      Run ETL jobs to see            â”‚
â”‚      visualization                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Loading State

During data fetch:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      âŸ³ Loading chart...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Responsive Behavior

- Chart width adjusts to container width on window resize
- Debounced re-render (250ms delay) to prevent excessive redraws
- Job name labels truncated on smaller screens

---

### Recent Runs Table

The Recent Runs table provides a detailed history of ETL job executions with client-side filtering, search, and pagination.

#### Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Recent Runs                                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                                          â”‚
â”‚  ğŸ” [Search by job name...        ]  [âœ• Clear Filters]         Status: [Completed] [Failed] [Cancelled] â”‚
â”‚                                                                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Run ID â”‚ ETL Job           â”‚ Connection         â”‚ Status    â”‚ Started         â”‚ Duration â”‚ Rows  â”‚ Act â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚  #567   â”‚ Daily Transactionsâ”‚ ğŸ˜ Prod PostgreSQL â”‚ âœ“ Completedâ”‚ Dec 26, 9:00 AMâ”‚ 37s      â”‚ 15,420â”‚ Viewâ”‚
â”‚  #566   â”‚ Weekly Products   â”‚ â˜ï¸ GCS Data Lake   â”‚ âœ“ Completedâ”‚ Dec 25, 8:00 AMâ”‚ 145s     â”‚ 5,230 â”‚ Viewâ”‚
â”‚  #565   â”‚ Hourly Inventory  â”‚ ğŸ˜ Prod PostgreSQL â”‚ âœ— Failed   â”‚ Dec 26, 2:30 PMâ”‚ 12s      â”‚ 0     â”‚ Viewâ”‚
â”‚  #564   â”‚ Monthly Report    â”‚ â˜ï¸ BigQuery        â”‚ âŠ˜ Cancelledâ”‚ Dec 24, 6:00 AMâ”‚ â€”        â”‚ 0     â”‚ Viewâ”‚
â”‚  #563   â”‚ Daily Transactionsâ”‚ ğŸ˜ Prod PostgreSQL â”‚ âœ“ Completedâ”‚ Dec 25, 9:00 AMâ”‚ 42s      â”‚ 14,892â”‚ Viewâ”‚
â”‚  #562   â”‚ Hourly Inventory  â”‚ ğŸ˜ Prod PostgreSQL â”‚ âš  Partial  â”‚ Dec 24, 3:30 PMâ”‚ 28s      â”‚ 3,100 â”‚ Viewâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Showing 1-6 of 102 runs                                    [Previous] [1] [2] ... [17] [Next]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Table Columns

| Column | Width | Description | Data Source |
|--------|-------|-------------|-------------|
| **Run ID** | 8% | Unique run identifier prefixed with # | `ETLRun.id` |
| **ETL Job** | 15% | Job name (truncated with tooltip) | `ETLRun.data_source.name` |
| **Connection** | 18% | Source type icon + connection name | `DataSource.connection.name` |
| **Status** | 12% | Status badge with icon | `ETLRun.status` |
| **Started** | 17% | Start timestamp (e.g., "Dec 26, 9:00 AM") | `ETLRun.started_at` |
| **Duration** | 10% | Execution time in seconds (or "â€”") | `ETLRun.duration_seconds` |
| **Rows** | 8% | Rows extracted with thousands separator | `ETLRun.total_rows_extracted` |
| **Actions** | 12% | "View Details" link | Opens run details modal |

#### Status Badges and Row Colors

| Status | Badge | Icon | Text Color | Row Background |
|--------|-------|------|------------|----------------|
| `completed` | `âœ“ Completed` | `fa-check-circle` | Green (`text-green-700`) | `bg-green-50` |
| `failed` | `âœ— Failed` | `fa-times-circle` | Red (`text-red-700`) | `bg-red-50` |
| `cancelled` | `âŠ˜ Cancelled` | `fa-ban` | Gray (`text-gray-600`) | `bg-gray-50` |
| `running` | `â†» Running` | `fa-spinner fa-spin` | Blue (`text-blue-700`) | `bg-blue-50` |
| `partial` | `âš  Partial` | `fa-exclamation-triangle` | Yellow (`text-yellow-700`) | `bg-yellow-50` |
| `pending` | `â± Pending` | `fa-clock` | Gray (`text-gray-600`) | `bg-gray-50` |

#### Connection Type Icons

| Source Type | Icon | Color |
|-------------|------|-------|
| `postgresql` | ğŸ˜ Elephant | Blue (`text-blue-600`) |
| `mysql` | Database | Orange (`text-orange-500`) |
| `bigquery` | Cloud | Blue (`text-blue-500`) |
| `gcs` | Cloud Upload | Yellow (`text-yellow-500`) |
| `s3` | AWS Logo | Orange (`text-orange-600`) |
| `firestore` | Fire | Yellow (`text-yellow-600`) |

---

#### Filter Controls

##### Search Input

- **Placeholder**: "Search by job name..."
- **Behavior**: Debounced search (200ms delay), immediate on Enter
- **Filter logic**: Case-insensitive substring match on `job_name`

##### Status Filter Buttons

Three toggle buttons for filtering by run status:

| Button | Default Style | Selected Style |
|--------|--------------|----------------|
| **Completed** | `bg-green-100 text-green-700` | `bg-green-500 text-white` |
| **Failed** | `bg-red-100 text-red-700` | `bg-red-500 text-white` |
| **Cancelled** | `bg-gray-200 text-gray-700` | `bg-gray-500 text-white` |

- **Multi-select**: Multiple statuses can be selected (OR logic)
- **Toggle behavior**: Click to select/deselect

##### Clear Filters Button

- **Visibility**: Hidden by default, shown when any filter is active
- **Action**: Clears search input and all status filters

---

#### Client-Side Filtering

The Recent Runs table uses **client-side filtering** for fast, responsive interaction.

```javascript
// Global state
let allRunsData = [];          // All runs from last 30 days (from embedded JSON)
let filteredRunsData = [];     // Runs matching current filters
let runsCurrentPage = 1;
const runsPerPage = 6;
let runsActiveStatuses = [];   // ['completed', 'failed', etc.]
let runsSearchQuery = '';

// Filter logic
filteredRunsData = allRunsData.filter(run => {
    // Search filter (case-insensitive)
    if (runsSearchQuery) {
        if (!run.job_name.toLowerCase().includes(runsSearchQuery.toLowerCase())) {
            return false;
        }
    }

    // Status filter (OR logic - match any selected status)
    if (runsActiveStatuses.length > 0) {
        if (!runsActiveStatuses.includes(run.status)) {
            return false;
        }
    }

    return true;
});
```

#### Data Structure

```javascript
// all_runs_json embedded in page (last 30 days)
[
    {
        "id": 567,
        "data_source_id": 42,
        "job_name": "Daily Transactions",
        "source_type": "postgresql",
        "connection_name": "Prod PostgreSQL",
        "status": "completed",
        "started_at": "2025-12-26T09:00:00+00:00",
        "duration_seconds": 37,
        "rows_extracted": 15420
    },
    // ... more runs
]
```

---

#### Data Scope

- **Time window**: Last 30 days (configurable in backend)
- **Query filter**: `started_at >= (now - 30 days) OR started_at IS NULL` (includes pending runs)
- **Status sync**: Running/pending runs are synchronized with Cloud Run status on page load

---

#### Pagination

- **Items per page**: 6
- **Page navigation**: Previous/Next buttons + page number links
- **Smart pagination**: Shows ellipsis for large page counts (e.g., [1] ... [5] [6] [7] ... [17])
- **Shows**: "Showing 1-6 of 102 runs" with "(filtered)" indicator when filters active

---

#### Empty States

Three distinct empty states based on context:

| Condition | Icon | Title | Subtitle |
|-----------|------|-------|----------|
| No runs ever | `fa-inbox` | "No ETL runs yet" | (empty) |
| Filters active, no matches | `fa-filter` | "No runs match your filters" | "Try adjusting your search or status filters" |
| Runs exist but none in 30 days | `fa-calendar-times` | "No runs in last 30 days" | "All recent ETL runs are older than 30 days" |

---

#### View Details Action

Clicking "View Details" opens a modal with comprehensive run information:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETL Run #567 Details                                        [X] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Job: Daily Transactions                                        â”‚
â”‚  Connection: Prod PostgreSQL (postgresql)                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Status      â”‚ âœ“ Completed                                â”‚   â”‚
â”‚  â”‚ Started     â”‚ Dec 26, 2025 9:00:00 AM                   â”‚   â”‚
â”‚  â”‚ Completed   â”‚ Dec 26, 2025 9:00:37 AM                   â”‚   â”‚
â”‚  â”‚ Duration    â”‚ 37 seconds                                 â”‚   â”‚
â”‚  â”‚ Rows        â”‚ 15,420                                     â”‚   â”‚
â”‚  â”‚ Bytes       â”‚ 2.4 MB                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  Error Message: (none)                                          â”‚
â”‚                                                                  â”‚
â”‚  [View Cloud Run Logs]                              [Close]     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ETL Run Cards

The Recent Runs section displays ETL runs as **card-based tablets** providing at-a-glance status, metrics, a 4-stage progress bar, and action buttons (View + Rerun).

#### Card Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Daily Transactions               #567â”‚  â”‚ Weekly Products              #566â”‚â”‚
â”‚ â”‚ ğŸ˜ Prod PostgreSQL                   â”‚  â”‚ â˜ï¸ GCS Data Lake                 â”‚â”‚
â”‚ â”‚ âœ“ Completed    Dec 26, 9:00 AM  37s  â”‚  â”‚ âœ“ Completed    Dec 25, 8:00  145sâ”‚â”‚
â”‚ â”‚                                      â”‚  â”‚                                  â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚ â”‚ â”‚  ROWS    â”‚  BYTES   â”‚  TABLES  â”‚  â”‚  â”‚ â”‚  ROWS    â”‚  BYTES   â”‚ TABLES â”‚â”‚â”‚
â”‚ â”‚ â”‚  15,420  â”‚  2.4 MB  â”‚    1     â”‚  â”‚  â”‚ â”‚  5,230   â”‚  892 KB  â”‚   1    â”‚â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚ â”‚                                      â”‚  â”‚                                  â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚ â”‚ â”‚  INIT  â”‚VALIDATEâ”‚EXTRACT â”‚ LOAD â”‚ â”‚  â”‚ â”‚  INIT  â”‚VALIDATEâ”‚EXTRACTâ”‚LOADâ”‚â”‚â”‚
â”‚ â”‚ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â”‚  â”‚ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Hourly Inventory                 #565â”‚  â”‚ Monthly Report              #564â”‚â”‚
â”‚ â”‚ ğŸ˜ Prod PostgreSQL                   â”‚  â”‚ â˜ï¸ BigQuery                      â”‚â”‚
â”‚ â”‚ âœ— Failed       Dec 26, 2:30 PM  12s  â”‚  â”‚ âŠ˜ Cancelled  Dec 24, 6:00 AM  â€” â”‚â”‚
â”‚ â”‚                                      â”‚  â”‚                                  â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚ â”‚ â”‚  ROWS    â”‚  BYTES   â”‚  TABLES  â”‚  â”‚  â”‚ â”‚  ROWS    â”‚  BYTES   â”‚ TABLES â”‚â”‚â”‚
â”‚ â”‚ â”‚    0     â”‚    â€”     â”‚    0     â”‚  â”‚  â”‚ â”‚    0     â”‚    â€”     â”‚   0    â”‚â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚ â”‚                                      â”‚  â”‚                                  â”‚â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚ â”‚ â”‚  INIT  â”‚VALIDATEâ”‚EXTRACT â”‚ LOAD â”‚ â”‚  â”‚ â”‚  INIT  â”‚VALIDATEâ”‚EXTRACTâ”‚LOADâ”‚â”‚â”‚
â”‚ â”‚ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–“â–“FAILâ–“â–“â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚ â”‚  â”‚ â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â”‚â”‚â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend (terminal statuses â€” completed/failed/partial/cancelled):
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ = Completed (purple gradient)
â–“â–“â–“â–“â–“â–“â–“â–“ = Failed (red)
â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ = Not reached (light gray)

Each card's actions column contains two buttons:
- **View** â€” Opens run details modal (`viewRunDetails(run.id)`)
- **Rerun** â€” Re-triggers the same ETL source (`runSourceNow(run.data_source_id)`), teal, 128px wide (`.card-action-btn.rerun.wide`)
  - **Enabled** (solid teal) for terminal states: `completed`, `failed`, `cancelled`, `partial`
  - **Disabled** (outline, 50% opacity) for `running` and `pending` states

Running/Pending jobs show an animated shimmer bar instead:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”            â”‚
â”‚ â”‚â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘ (shimmer)â”‚ EXTRACTINGâ”‚
â”‚ â””â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4-Stage Pipeline Architecture

ETL runs follow a 4-stage pipeline model for accurate progress tracking and error localization:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETL 4-Stage Pipeline                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚   INIT   â”‚â”€â”€â”€â–¶â”‚  VALIDATE â”‚â”€â”€â”€â–¶â”‚  EXTRACT  â”‚â”€â”€â”€â–¶â”‚   LOAD   â”‚           â”‚
â”‚   â”‚          â”‚    â”‚           â”‚    â”‚           â”‚    â”‚          â”‚           â”‚
â”‚   â”‚ Config   â”‚    â”‚ BQ Table  â”‚    â”‚ Source    â”‚    â”‚ BigQuery â”‚           â”‚
â”‚   â”‚ Loading  â”‚    â”‚ Exists?   â”‚    â”‚ Query     â”‚    â”‚ Write    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚        â”‚               â”‚                â”‚                â”‚                  â”‚
â”‚        â–¼               â–¼                â–¼                â–¼                  â”‚
â”‚   init_completed  validation_    extraction_       loading_                 â”‚
â”‚   _at             completed_at   started/          started/                 â”‚
â”‚                                  completed_at      completed_at             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stage Details:**

| Stage | Description | Timestamp Fields | Typical Duration |
|-------|-------------|------------------|------------------|
| **INIT** | Load configuration, establish connections, validate job config | `init_completed_at` | <1s |
| **VALIDATE** | Verify BigQuery destination table exists and is accessible | `validation_completed_at` | 1-2s |
| **EXTRACT** | Pull data from source (database, files, API) | `extraction_started_at`, `extraction_completed_at` | Variable |
| **LOAD** | Write data to BigQuery | `loading_started_at`, `loading_completed_at` | Variable |

#### Error Type Classification

When an ETL run fails, the system classifies the error to identify which stage failed:

```python
# Error type constants (etl_runner/utils/error_handling.py)
ERROR_TYPE_INIT = 'init'           # Configuration, credentials, connection setup
ERROR_TYPE_VALIDATION = 'validation'  # Table doesn't exist, schema mismatch
ERROR_TYPE_EXTRACTION = 'extraction'  # Query failed, source unavailable
ERROR_TYPE_LOAD = 'load'           # BigQuery write failed
ERROR_TYPE_UNKNOWN = 'unknown'     # Unclassified errors
```

**Error Classification Logic:**

| Error Pattern | Classified As | Examples |
|---------------|---------------|----------|
| `ConfigurationError` exception | `init` | Missing required config, invalid source type |
| "credentials", "authentication" in message | `init` | Invalid password, expired token |
| "not found", "does not exist" | `validation` | BQ table missing, collection not found |
| "destination table does not exist" | `validation` | BQ table not created yet |
| `ExtractionError` exception | `extraction` | Query timeout, source offline |
| "query failed", "source unavailable" | `extraction` | Database connection lost |
| `LoadError` exception | `load` | BQ insert failed, quota exceeded |
| "bigquery write/insert failed" | `load` | Schema mismatch during load |

**Database Schema:**

```python
class ETLRun(models.Model):
    # ... existing fields ...

    ERROR_TYPE_CHOICES = [
        ('init', 'Initialization'),
        ('validation', 'Validation'),
        ('extraction', 'Extraction'),
        ('load', 'Load'),
        ('unknown', 'Unknown'),
    ]

    error_type = models.CharField(
        max_length=20,
        choices=ERROR_TYPE_CHOICES,
        blank=True,
        help_text="Type of error that caused failure (init, validation, extraction, load)"
    )

    # 4-stage pipeline timestamps
    init_completed_at = models.DateTimeField(null=True, blank=True)
    validation_completed_at = models.DateTimeField(null=True, blank=True)
    extraction_started_at = models.DateTimeField(null=True, blank=True)
    extraction_completed_at = models.DateTimeField(null=True, blank=True)
    loading_started_at = models.DateTimeField(null=True, blank=True)
    loading_completed_at = models.DateTimeField(null=True, blank=True)
```

#### Progress Bar Rendering Logic

`renderRunStagesBar(run)` branches based on run status:

**Running/Pending jobs** â€” Animated indeterminate indicator with current phase text:

```javascript
// For running/pending: animated shimmer bar with phase label
if (run.status === 'running' || run.status === 'pending') {
    let phaseLabel = 'Initializing...';
    if (run.loading_started_at && !run.loading_completed_at) phaseLabel = 'Loading...';
    else if (run.extraction_started_at && !run.extraction_completed_at) phaseLabel = 'Extracting...';
    else if (run.init_completed_at && !run.validation_completed_at) phaseLabel = 'Validating...';
    else if (run.init_completed_at && run.validation_completed_at) phaseLabel = 'Extracting...';

    return `
        <div class="run-stages-running">
            <div class="run-stages-running-bar"></div>
            <div class="run-stages-running-label">${phaseLabel}</div>
        </div>
    `;
}
```

**Terminal statuses (completed, failed, partial, cancelled)** â€” 4-segment diagnostic bar (unchanged):

```javascript
// For terminal statuses: 4-segment bar with error_type classification
let initStatus = 'pending';
// ... timestamp-based detection + error_type classification (same as before)

return `
    <div class="run-card-stages">
        <div class="run-stages-bar">
            <div class="run-stage-segment ${initStatus}">Init</div>
            <div class="run-stage-segment ${validateStatus}">Validate</div>
            <div class="run-stage-segment ${extractStatus}">Extract</div>
            <div class="run-stage-segment ${loadStatus}">Load</div>
        </div>
    </div>
`;
```

When a running job completes (detected via polling), the animated indicator automatically transitions to the 4-segment diagnostic bar.

#### CSS Styling

```css
/* Progress Bar (4 stages: INIT â†’ VALIDATE â†’ EXTRACT â†’ LOAD) â€” terminal statuses only */
.run-stages-bar { display: flex; height: 24px; border-radius: 6px; overflow: hidden; background: #e5e7eb; }
.run-stage-segment { flex: 1; display: flex; align-items: center; justify-content: center; font-size: 9px; font-weight: 500; color: white; text-transform: uppercase; }
.run-stage-segment.completed { background: linear-gradient(135deg, #6366f1 0%, #818cf8 100%); }
.run-stage-segment.failed { background: #ef4444; }
.run-stage-segment.pending { background: #d1d5db; color: #6b7280; }
.run-stage-segment.not-reached { background: #e5e7eb; color: #9ca3af; }

/* Running/Pending animated indicator */
.run-stages-running { display: flex; align-items: center; gap: 10px; width: 100%; margin-top: 12px; padding-top: 12px; border-top: 1px solid #f3f4f6; }
.run-stages-running-bar { flex: 1; height: 22px; border-radius: 6px; overflow: hidden; background: #e5e7eb; position: relative; }
.run-stages-running-bar::after {
    content: ''; position: absolute; inset: 0;
    background: linear-gradient(90deg, #818cf8, #6366f1, #4f46e5, #6366f1, #818cf8, #6366f1);
    background-size: 200% 100%;
    animation: run-shimmer 1.8s linear infinite;
}
@keyframes run-shimmer { 0% { background-position: 200% 0; } 100% { background-position: -200% 0; } }
.run-stages-running-label { font-size: 11px; font-weight: 600; color: #6366f1; text-transform: uppercase; letter-spacing: 0.05em; white-space: nowrap; min-width: 100px; }
```

#### ETL Runner Phase Reporting

The ETL runner reports timestamps at each phase transition:

```python
# etl_runner/main.py

class ETLRunner:
    def _report_phase_timestamp(self, phase: str):
        """Report a phase timestamp to Django API."""
        if self.etl_run_id:
            self.config.update_etl_run_status(
                etl_run_id=self.etl_run_id,
                status='running',
                **{f'{phase}_at': True}
            )

    def run(self):
        self._current_phase = 'init'

        # PHASE 1: INIT
        # (config loaded in __init__)
        self._report_phase_timestamp('init_completed')
        self._current_phase = 'validation'

        # PHASE 2: VALIDATE
        if not self.loader.verify_table_exists():
            raise ConfigurationError("Destination table does not exist")
        self._report_phase_timestamp('validation_completed')
        self._current_phase = 'extraction'

        # PHASE 3: EXTRACT
        self._report_phase_timestamp('extraction_started')
        # ... extraction logic ...
        self._report_phase_timestamp('extraction_completed')
        self._current_phase = 'load'

        # PHASE 4: LOAD
        self._report_phase_timestamp('loading_started')
        # ... loading logic ...
        self._report_phase_timestamp('loading_completed')
```

**Error Handling with Phase Context:**

```python
# On failure, error is classified based on current phase
except Exception as e:
    phase_to_error_type = {
        'init': ERROR_TYPE_INIT,
        'validation': ERROR_TYPE_VALIDATION,
        'extraction': ERROR_TYPE_EXTRACTION,
        'load': ERROR_TYPE_LOAD,
    }
    error_type = phase_to_error_type.get(self._current_phase)
    handle_etl_error(e, self.config, self.etl_run_id, error_type=error_type)
```

#### Bytes Processed Tracking

ETL runs now track `bytes_processed` for KPI display:

```python
# In counting_generator() for each load method:
def counting_generator(gen):
    for df in gen:
        self.total_rows_extracted += len(df)
        # Track bytes processed (DataFrame memory usage)
        self.total_bytes_processed += df.memory_usage(deep=True).sum()
        yield df

# For file sources, also add file size:
self.total_bytes_processed += file_metadata.get('file_size_bytes', 0)
```

#### JSON Response Structure

The view includes all fields needed for card rendering:

```python
# ml_platform/etl/views.py - runs_json_list
run_data = {
    'id': run.id,
    'status': run.status,
    'error_type': run.error_type or '',  # For failed run coloring
    'job_name': run.data_source.name,
    'connection_name': run.data_source.connection.name,
    'source_type': run.data_source.source_type,
    'started_at': run.started_at.isoformat(),
    'duration_seconds': run.get_duration_seconds(),
    'rows_extracted': run.total_rows_extracted,
    'rows_loaded': run.rows_loaded,
    'bytes_processed': run.bytes_processed,
    # 4-stage pipeline timestamps
    'init_completed_at': run.init_completed_at.isoformat() if run.init_completed_at else None,
    'validation_completed_at': run.validation_completed_at.isoformat() if run.validation_completed_at else None,
    'extraction_started_at': run.extraction_started_at.isoformat() if run.extraction_started_at else None,
    'extraction_completed_at': run.extraction_completed_at.isoformat() if run.extraction_completed_at else None,
    'loading_started_at': run.loading_started_at.isoformat() if run.loading_started_at else None,
    'loading_completed_at': run.loading_completed_at.isoformat() if run.loading_completed_at else None,
}
```

#### ETL Run Logs

The View modal includes a "Load Logs" feature that fetches Cloud Run Job logs from Cloud Logging. This allows users to debug ETL runs without needing direct access to GCP Console.

**Log Query Strategy:**

1. **Primary (by execution_name):** Most precise, uses the Cloud Run execution name
   ```
   resource.type = "cloud_run_job"
   resource.labels.job_name = "etl-runner"
   labels."run.googleapis.com/execution_name" = "{execution_name}"
   resource.labels.location = "europe-central2"
   severity >= DEFAULT
   ```

2. **Fallback (by timestamp range):** Used when execution_name is not available
   ```
   resource.type = "cloud_run_job"
   resource.labels.job_name = "etl-runner"
   resource.labels.location = "europe-central2"
   timestamp >= "{started_at - 1 minute}"
   timestamp <= "{completed_at + 5 minutes}"
   severity >= DEFAULT
   ```

**Database Schema:**

```python
class ETLRun(models.Model):
    # ... existing fields ...

    # For log queries
    cloud_run_execution_name = models.CharField(
        max_length=255,
        blank=True,
        help_text="Cloud Run Job execution name (e.g., 'etl-runner-abc123') for log queries"
    )
```

**Service Implementation:**

```python
# ml_platform/etl/etl_logs_service.py

class EtlLogsService:
    """Fetches Cloud Run Job logs from Cloud Logging."""

    def get_logs(self, etl_run, limit=100) -> Dict:
        """
        Returns:
            {
                'available': bool,
                'logs': [{'timestamp': str, 'severity': str, 'message': str}, ...],
                'count': int,
                'source': 'execution' | 'timestamp_range',
                'message': str  # Only if available=False
            }
        """
```

**API Endpoint:**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/etl/runs/{run_id}/logs/` | Fetch logs for an ETL run |

**Query Parameters:**
- `limit` (optional): Maximum number of log entries (default: 100)

**Response Example:**
```json
{
    "available": true,
    "logs": [
        {"timestamp": "2026-02-02 14:30:15", "severity": "INFO", "message": "ETL RUN STARTED"},
        {"timestamp": "2026-02-02 14:30:16", "severity": "INFO", "message": "PHASE: INIT completed"},
        {"timestamp": "2026-02-02 14:30:17", "severity": "ERROR", "message": "ConfigurationError: Table not found"}
    ],
    "count": 3,
    "source": "execution"
}
```

**Frontend UI:**

The View modal includes a logs section with:
- "Load Logs" button to fetch logs on demand
- Dark terminal-style display
- Severity color coding:
  - ERROR/CRITICAL/ALERT/EMERGENCY â†’ Red
  - WARNING â†’ Yellow
  - INFO â†’ Blue
  - DEBUG/DEFAULT â†’ Gray

**Execution Name Capture:**

When an ETL job is triggered (manual or scheduled), the execution name is extracted from the Cloud Run Jobs API response:

```python
# ml_platform/etl/api.py - run_source()
operation = client.run_job(request=exec_request)

# Extract execution name from operation metadata
if hasattr(operation, 'metadata') and operation.metadata:
    metadata_name = getattr(operation.metadata, 'name', '')
    if metadata_name and '/executions/' in metadata_name:
        execution_name = metadata_name.split('/executions/')[-1]

etl_run.cloud_run_execution_name = execution_name or ''
```

---

## Known Issues and Fixes

This section documents bugs discovered during ETL system usage and their fixes.

### Issue #1: Dataflow Jobs Reporting 0 Rows (Fixed 2025-12-26)

**Symptoms:**
- ETL jobs using Dataflow completed successfully (data was loaded to BigQuery)
- But the UI showed "Rows Extracted: 0, Rows Loaded: 0"
- Logs showed: `Failed to wait for Dataflow completion: cannot import name 'dataflow_v1beta3' from 'google.cloud'`

**Root Cause:**
1. Missing `google-cloud-dataflow-client` package in `etl_runner/requirements.txt`
2. No fallback logic when Dataflow API calls failed

**Fix Applied:**
1. Added `google-cloud-dataflow-client>=0.8.6` to ETL runner requirements
2. Added fallback logic in `main.py` to use estimated row counts when API fails:
   ```python
   # Use estimated rows as fallback if Dataflow API failed or returned 0
   effective_rows = final_rows_loaded
   if final_rows_loaded == 0 and estimated_rows > 0:
       effective_rows = estimated_rows
   ```
3. Added retry logic (3 attempts) for Dataflow client initialization and job listing

**Files Modified:**
- `etl_runner/requirements.txt` - Added dependency
- `etl_runner/main.py` - Added fallback logic and retry handling in `run_with_dataflow()` and `_wait_for_dataflow_completion()`

---

### Issue #2: Incorrect ETL Run Status for Dataflow Jobs (Fixed 2025-12-26)

**Symptoms:**
- UI showed "Dataflow failed with errors" when Dataflow was still running
- Status was incorrectly updated based on Cloud Run execution status, not Dataflow job status

**Root Cause:**
- `sync_running_etl_runs_with_cloud_run()` in `views.py` synced with Cloud Run execution status
- Cloud Run completes quickly after submitting a Dataflow job
- If Cloud Run showed any error, the ETL run was marked as failed even though Dataflow was still running successfully

**Fix Applied:**
1. Added `dataflow_job_id` field to `ETLRun` model to track Dataflow jobs separately
2. Created new `sync_running_etl_runs_with_dataflow()` function that queries Dataflow API directly
3. Updated `model_etl` view to prioritize Dataflow status sync over Cloud Run sync

**Files Modified:**
- `ml_platform/models.py` - Added `dataflow_job_id` field to `ETLRun`
- `ml_platform/migrations/0040_add_dataflow_job_id_to_etlrun.py` - Migration for new field
- `ml_platform/etl/views.py` - Added `sync_running_etl_runs_with_dataflow()` function
- `ml_platform/etl/api.py` - Updated `run_update()` and `run_status()` to handle `dataflow_job_id`
- `etl_runner/main.py` - Pass `dataflow_job_id` in result and status updates
- `etl_runner/config.py` - Updated docstring for `dataflow_job_id` parameter

**New Field Schema:**
```python
class ETLRun(models.Model):
    # ... existing fields ...
    dataflow_job_id = models.CharField(
        max_length=255,
        blank=True,
        help_text="Dataflow job ID for large-scale ETL runs (used for accurate status tracking)"
    )
```

---

### Issue #3: Scheduler Job Not Appearing in UI (Analyzed 2025-12-26)

**Symptoms:**
- When creating a job with "run immediately" checked, the scheduler didn't appear in "Scheduled Jobs" table
- Scheduler appeared only after page refresh post job completion

**Analysis:**
- Investigation confirmed the scheduler **was** created at job creation time (verified in logs)
- The issue was timing - the page may have been loaded before `cloud_scheduler_job_name` was saved
- **This is not a code bug** - it's a normal page refresh timing issue

**Recommendation:**
- Consider adding real-time updates to the scheduled jobs table (WebSocket or polling)
- No code fix required

---

### Issue #4: Schema Detection 1GB File Size Limit (Fixed 2025-12-26)

**Symptoms:**
- ETL wizard refused to detect schema for CSV files larger than 1GB
- Error message: "File size (3.74 GB) exceeds 1GB limit"
- Files were visible in the wizard but "Detect Schema" button failed

**Root Cause:**
- Hardcoded 1GB limit in `detect_file_schema()` function in `ml_platform/connections/api.py`
- The limit was unnecessary because schema detection only downloads first 5MB of the file
- This was defensive code added before Dataflow support was implemented

**Fix Applied:**
Removed the arbitrary 1GB file size check. The 5MB sample download was already safe for files of any size:

```python
# REMOVED:
if file_size > 1024 * 1024 * 1024:  # 1GB
    return JsonResponse({'status': 'error', 'message': f'File size exceeds 1GB limit'})

# KEPT (already safe):
max_bytes_to_download = min(file_size, 5 * 1024 * 1024)  # 5MB max
file_content = blob.download_as_bytes(end=max_bytes_to_download)
```

**Files Modified:**
- `ml_platform/connections/api.py` - Removed 1GB limit in `detect_file_schema()` (lines 1330-1334)

---

### Issue #5: Dataflow Worker OOM with Large CSV Files (Fixed 2025-12-26)

**Symptoms:**
- Dataflow jobs failed when processing large CSV files (4GB+)
- Error: "Timed out waiting for an update from the worker"
- Error: "The worker has been reported dead"
- Jobs failed after 4 retry attempts

**Root Cause:**
The Dataflow pipeline used **pandas inside workers** to process files, which loaded entire files into memory:

```python
# OLD (broken) - in UnifiedExtractor._process_file():
df = extractor.extract_file(file_path)  # Downloads entire 4GB file
for _, row in df.iterrows():            # Requires 12-15GB RAM
    yield row
```

For a 4GB CSV file:
- Pandas needs ~12-15GB RAM to load and parse
- Workers were configured with `n1-standard-2` (7.5GB RAM)
- Workers ran out of memory and were killed

**Fix Applied:**
Implemented **native Beam I/O** for file sources, replacing pandas with streaming:

1. **New `ParseCSVLine` DoFn** - Parses CSV lines using Python's `csv` module (handles quotes, escapes)
2. **New `ParseJSONLine` DoFn** - Parses JSON lines using `json.loads()`
3. **New `run_file_pipeline()` function** - Uses `beam.io.ReadFromText` for streaming

```python
# NEW (fixed) - Native Beam I/O:
pipeline
| 'ReadCSV' >> beam.io.ReadFromText(
    file_pattern='gs://bucket/file.csv',
    skip_header_lines=1
)
# Beam automatically splits into ~64MB bundles
# Each worker processes bundles in parallel
# Memory usage: ~100-200MB per worker (not 12GB!)

| 'ParseCSVLines' >> beam.ParDo(ParseCSVLine(column_names=schema_columns))
| 'SerializeValues' >> beam.Map(...)
| 'WriteToBigQuery' >> WriteToBigQuery(...)
```

**Key Benefits:**
| Aspect | Before | After |
|--------|--------|-------|
| Memory per worker | 12-15GB | ~100-200MB |
| File size limit | ~2GB | Unlimited |
| Processing model | Load entire file | Stream line-by-line |
| File splitting | Manual | Automatic (~64MB bundles) |

**Pipeline Selection Logic:**
```python
# In main.py run_with_dataflow():
if source_type == 'bigquery':
    run_bigquery_native_pipeline(...)  # Native BigQuery I/O
elif is_file_source:
    run_file_pipeline(...)             # NEW - Native Beam I/O
else:
    run_scalable_pipeline(...)         # Database sources
```

**Error Handling:**
- Bad CSV records (parsing errors, field count mismatch) are skipped and logged
- Skipped records are counted in Dataflow metrics (visible in GCP Console)
- Pipeline continues processing valid records

**Files Modified:**
- `etl_runner/dataflow_pipelines/etl_pipeline.py`:
  - Added `ParseCSVLine` DoFn (lines 297-381)
  - Added `ParseJSONLine` DoFn (lines 384-443)
  - Added `run_file_pipeline()` function (lines 1343-1575)
  - Added imports for `csv` and `io` modules
- `etl_runner/main.py`:
  - Updated `run_with_dataflow()` to use `run_file_pipeline()` for file sources
  - Added full GCS path construction from bucket name and file paths

---

### Issue #6: CSV Column Mapping and TIMESTAMP Parsing Failures (Fixed 2025-12-27)

**Symptoms:**
- Dataflow job fails with: "JSON table encountered too many errors, giving up. Rows: 1; errors: 1"
- Error occurs at BigQuery FILE_LOADS stage (not during CSV parsing)
- All rows fail with the same error

**Root Causes:**

1. **Empty `column_mapping` for legacy ETL jobs:**
   - The `column_mapping` field was added to `DataSourceTable` model after some ETL jobs were created
   - Legacy jobs have `column_mapping = NULL`
   - When empty, the code fell back to BigQuery schema column order (wrong order for CSV positional parsing)

2. **Time-only values in TIMESTAMP columns:**
   - CSV had `invoice_time` column with values like `"18:53:06"` (time only, no date)
   - BigQuery TIMESTAMP requires full datetime format
   - The `SchemaAwareConverter` only handled date-only strings (`"YYYY-MM-DD"`), not time-only

3. **Extra CSV columns not in BigQuery schema:**
   - CSV may have more columns than BigQuery table (user selected subset in wizard)
   - All CSV columns were included in JSON output, causing BigQuery load failures

**Solution:**

1. **Read CSV header at runtime (`read_csv_header_from_gcs`):**
   ```python
   # Read first 64KB from GCS file to get header
   # Parse with csv.reader for proper handling of quoted fields
   # Returns column names in correct file order
   ```

2. **Smart column matching (`apply_column_mapping`):**
   ```python
   # If column_mapping exists: use it
   # If empty: match CSV columns to BigQuery schema by sanitized name
   # Fuzzy matching handles "CHANNEL_DESC" -> "channel_desc"
   ```

3. **Filter to BigQuery schema columns:**
   ```python
   # After parsing, remove any columns not in BigQuery schema
   | 'FilterToSchemaColumns' >> beam.Map(
       lambda row, keep_cols=columns_to_keep: {k: v for k, v in row.items() if k in keep_cols}
   )
   ```

4. **Handle time-only TIMESTAMP values:**
   ```python
   # In SchemaAwareConverter:
   # "18:53:06" (time-only) -> "1970-01-01T18:53:06"
   # "2024-02-22" (date-only) -> "2024-02-22T00:00:00"
   ```

**Files Modified:**
- `etl_runner/dataflow_pipelines/etl_pipeline.py`:
  - Added `read_csv_header_from_gcs()` function (lines 384-478)
  - Added `apply_column_mapping()` function with BQ schema fallback (lines 481-547)
  - Updated `run_file_pipeline()` to read header and filter columns (lines 1591-1617)
  - Added `FilterToSchemaColumns` pipeline step (lines 1757-1761)
  - Enhanced `SchemaAwareConverter` for time-only timestamps (lines 268-285)

**Verification:**
```bash
# Check CSV header
gsutil cat -r 0-500 gs://bucket/file.csv | head -1

# Check BigQuery schema
bq show --schema --format=prettyjson project:dataset.table

# Check ETL job config
python manage.py shell -c "
from ml_platform.models import DataSourceTable
t = DataSourceTable.objects.get(dest_table_name='table_name')
print(f'column_mapping: {t.column_mapping}')
print(f'selected_columns: {t.selected_columns}')
"
```

**Prevention:**
- New ETL jobs created after fix will have `column_mapping` populated
- Consider backfilling `column_mapping` for existing jobs
- Add validation in wizard for ambiguous timestamp formats

---

### Issue #7: Dataflow Transactional Loads Reporting Total Table Row Count (Fixed 2026-02-07)

**Symptoms:**
- ETL jobs using Dataflow with **transactional** (append) load type showed incorrect ROWS and LOADED values
- UI displayed the **total BigQuery table row count** (e.g., 55,869,500) instead of rows loaded in that run
- SIZE showed "0 B" for Dataflow runs
- Only affected Dataflow-processed runs; standard (pandas) processing reported correctly

**Root Cause:**

When a Dataflow job completed, `_wait_for_dataflow_completion()` called `_get_bigquery_row_count()` which ran `SELECT COUNT(*) FROM destination_table` â€” returning the **entire table's row count**, not just the rows appended by this run:

```python
# OLD (broken) - _get_bigquery_row_count() returns total table rows
def _get_bigquery_row_count(self) -> int:
    query = f"SELECT COUNT(*) as cnt FROM `{table_ref}`"  # 55M+ for existing tables
    ...
```

For catalog (replace) loads this was correct (table is replaced, so total = loaded). But for transactional (append) loads, the pre-existing rows were included in the count.

The `estimated_rows` fallback (from Issue #1 fix) didn't trigger because `final_rows_loaded` was non-zero (55M+, not 0).

Additionally, `total_bytes_processed` stayed at 0 because byte tracking happens in-process via DataFrame memory usage, but Dataflow workers process data independently.

**Fix Applied:**

Snapshot-delta approach â€” capture BigQuery row count before and after Dataflow execution, report the difference:

```python
# 1. Snapshot BEFORE Dataflow launch
pre_load_row_count = self._get_bigquery_row_count()

# 2. Dataflow runs...

# 3. After completion, _wait_for_dataflow_completion() returns post_load_row_count

# 4. For transactional loads, compute delta
if load_type == 'transactional' and post_load_row_count > 0:
    actual_rows_loaded = post_load_row_count - pre_load_row_count
    final_rows_loaded = actual_rows_loaded  # e.g., 55M - 50M = 5M
else:
    final_rows_loaded = post_load_row_count  # Catalog: total is correct
```

Also fixed SIZE: 0 B by summing file sizes from `files_to_process` metadata:
```python
if is_file_source and files_to_process:
    self.total_bytes_processed = sum(f.get('file_size_bytes', 0) for f in files_to_process)
```

**Edge Cases Handled:**
- Non-positive delta (e.g., concurrent table modifications): falls back to `estimated_rows`
- Pre-load count query failure (returns 0): delta equals post-count, same as previous behavior â€” no regression
- Catalog loads: unaffected, uses post-count directly as before

**Files Modified:**
- `etl_runner/main.py` - Three changes in `run_with_dataflow()`:
  1. Added pre-load row count snapshot before Dataflow pipeline launch
  2. Replaced post-Dataflow row count logic with snapshot-delta calculation for transactional loads
  3. Added bytes tracking from `files_to_process` file size metadata

### Issue #8: Frozen Progress Bar and Broken Polling for ETL Run Cards (Fixed 2026-02-07)

**Symptoms:**
- Running/pending ETL jobs displayed a 4-segment progress bar (Init â†’ Validate â†’ Extract â†’ Load) that rendered once on page load and never updated
- The bar always showed a frozen state because ETL jobs run as Cloud Run Jobs that complete in seconds with no intermediate progress API
- Polling never fired because `startRunningJobsPolling()` queried for `tr[data-run-status="running"]` table rows, but the UI uses card-based layout (no `<tr>` elements)
- Even if polling had fired, the API response was missing `init_completed_at`, `validation_completed_at`, and `error_type` fields needed by `renderRunStagesBar()`

**Root Cause:**

Three independent issues:
1. **Wrong UI paradigm for running jobs:** The 4-segment bar was modeled after Vertex AI Pipeline progress, but Cloud Run Jobs have no intermediate progress API â€” the bar rendered once and stayed frozen
2. **Polling targeted wrong DOM elements:** `document.querySelectorAll('tr[data-run-status="running"]')` found nothing because run cards use `<div class="run-card">`, not `<tr>` elements
3. **Incomplete polling API:** `run_status()` endpoint in `api.py` was missing phase timestamp fields that the page-load data from `views.py` included

**Fix Applied:**

1. **New animated indicator for running/pending jobs:** `renderRunStagesBar()` now branches on status â€” running/pending jobs show a CSS shimmer bar with current phase text ("Initializing...", "Validating...", "Extracting...", "Loading...") instead of the 4-segment bar. Terminal statuses (completed, failed, partial, cancelled) keep the existing 4-segment diagnostic bar unchanged.

2. **Fixed polling to use data array:** `startRunningJobsPolling()` and `pollRunningJobs()` now filter `allRunsData` instead of querying DOM for table rows.

3. **New `updateRunCard()` function:** Replaces `updateRunRow()` for card-based updates â€” merges API data into `allRunsData`, updates status icon, duration, metrics, and re-renders the stages bar. When status transitions to terminal, the animated indicator naturally becomes the 4-segment diagnostic bar.

4. **Added missing API fields:** `run_status()` now returns `init_completed_at`, `validation_completed_at`, and `error_type`, aligning with the page-load data from `views.py`.

**Files Modified:**
- `ml_platform/etl/api.py` â€” Added 3 missing fields to `run_status()` response
- `templates/ml_platform/model_etl.html` â€” New CSS for animated indicator, branching `renderRunStagesBar()`, fixed polling functions, new `updateRunCard()`

---

## Files Reference

### Backend Files

| File | Purpose |
|------|---------|
| `ml_platform/models.py` | Connection, DataSource, DataSourceTable, ETLRun, ProcessedFile models |
| `ml_platform/connections/urls.py` | Connection API URL routes |
| `ml_platform/connections/api.py` | Connection API endpoint handlers |
| `ml_platform/etl/urls.py` | ETL API URL routes |
| `ml_platform/etl/api.py` | ETL job API endpoint handlers |
| `ml_platform/etl/views.py` | ETL page view |
| `ml_platform/etl/webhooks.py` | Cloud Scheduler webhook handler |
| `ml_platform/utils/connection_manager.py` | Connection testing and metadata fetching |
| `ml_platform/utils/cloud_scheduler.py` | Cloud Scheduler management |

### ETL Runner Microservice

| File | Purpose |
|------|---------|
| `etl_runner/main.py` | Entry point, ETLRunner orchestrator class |
| `etl_runner/config.py` | Configuration management, Django API client |
| `etl_runner/extractors/postgresql.py` | PostgreSQL data extraction |
| `etl_runner/extractors/mysql.py` | MySQL data extraction |
| `etl_runner/extractors/bigquery.py` | BigQuery data extraction |
| `etl_runner/extractors/firestore.py` | Firestore data extraction |
| `etl_runner/extractors/file_extractor.py` | GCS/S3/Azure file extraction |
| `etl_runner/loaders/bigquery_loader.py` | BigQuery loading (batch + streaming) |
| `etl_runner/dataflow_pipelines/etl_pipeline.py` | Apache Beam pipeline definitions |
| `etl_runner/dataflow_pipelines/partitioning.py` | Work unit calculation for parallel processing |
| `etl_runner/utils/logging_config.py` | Structured JSON logging |
| `etl_runner/utils/error_handling.py` | Error handling and status updates |

### Frontend Files

| File | Purpose |
|------|---------|
| `templates/ml_platform/model_etl.html` | Main ETL page template |
| `static/css/cards.css` | Card styling for connections/jobs |
| `static/css/modals.css` | Modal styling for wizards |

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| v18 | 2026-02-07 | ETL Run Cards: Added Rerun button to run cards with `data_source_id` in run JSON; reuses `runSourceNow()` and existing trigger endpoint; disabled for running/pending states |
| v17 | 2026-02-07 | ETL Run Cards: Replaced frozen 4-segment bar with animated shimmer indicator for running/pending jobs; fixed polling to target cards instead of table rows; added missing API fields |
| v16 | 2026-02-03 | ETL Jobs: Added filter bar (Status, Connection, Schedule) matching ETL Runs style |
| v15 | 2026-02-03 | ETL Jobs: 6 per page, 2-column layout, fixed card height with `items-start` |
| v14 | 2026-02-03 | Restyled filter bar (matches ETL Runs), added Status filter (Working/Failed/Not Tested) |
| v13 | 2026-02-03 | Fixed card height stretching, pagination at 6 cards per page (2 rows Ã— 3 columns) |
| v12 | 2026-02-03 | Added type filter dropdown (Relational/Cloud Storage/NoSQL), changed to 3-column card layout |
| v11 | 2026-02-03 | Split ETL Setup into separate Connections and ETL Jobs chapters for cleaner UI |
| v10 | 2026-02-02 | Added in-app logs viewer for ETL runs |
| v9 | 2026-02-02 | Added ETL Run Cards section with 4-stage pipeline (INITâ†’VALIDATEâ†’EXTRACTâ†’LOAD), error type classification, and bytes tracking |
| v8 | 2025-12-27 | Added Issue #6 (CSV column mapping and TIMESTAMP parsing fixes for Dataflow) |
| v7 | 2025-12-26 | Added Issue #4 (1GB schema limit) and Issue #5 (Dataflow OOM fix with native Beam I/O) |
| v6 | 2025-12-26 | Added Known Issues and Fixes section (Dataflow row count, status tracking) |
| v5 | 2025-12-26 | Added Recent Runs table documentation |
| v4 | 2025-12-26 | Added Scheduled Jobs table and Bubble Chart documentation |
| v3 | 2025-12-26 | Added ETL Jobs Dashboard chapter with KPI documentation |
| v2 | 2025-12-26 | Added ETL Jobs sub-chapter with full documentation |
| v1 | 2025-12-26 | Initial documentation for Connections sub-chapter |
