# Phase: Training Domain

## Document Purpose
This document provides detailed specifications for implementing the **Training** domain in the ML Platform. The Training domain executes full TFX pipelines for production model training.

**Last Updated**: 2025-12-01

---

## Overview

### Purpose
The Training domain allows users to:
1. Select a Dataset + Feature Config combination
2. Configure training hyperparameters (epochs, batch size, etc.)
3. Execute a full TFX pipeline via Vertex AI Pipelines
4. Monitor pipeline progress in real-time
5. Track artifacts in ML Metadata

### Key Principle
**Training is the production execution of a validated configuration.** Users should run Quick Tests in Modeling first, then promote the best config to Full Training.

### Output
- Trained TFRS model (SavedModel with embedded Transform)
- TFX artifacts tracked in ML Metadata
- Metrics logged to MLflow
- Model ready for deployment

---

## User Interface

### Training Runs List View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Runs                                              [+ New Training] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ ðŸ”„ Training Run #47                                           Running â”‚  â”‚
â”‚ â”‚ Dataset: Q4 2024 Training Data | Config: config-042 (Large embeddings) â”‚  â”‚
â”‚ â”‚ Started: 45 min ago | Stage: Trainer (epoch 8/20)                      â”‚  â”‚
â”‚ â”‚ [View Progress]                                                        â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ âœ… Training Run #46                                        Completed   â”‚  â”‚
â”‚ â”‚ Dataset: Q4 2024 Training Data | Config: config-038                    â”‚  â”‚
â”‚ â”‚ Duration: 3h 42m | Cost: $38.50 | Recall@100: 46.8%                   â”‚  â”‚
â”‚ â”‚ [View Results] [Deploy] [Compare]                                      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ âœ… Training Run #45                                        Deployed â—  â”‚  â”‚
â”‚ â”‚ Dataset: Q3 2024 Training Data | Config: config-032                    â”‚  â”‚
â”‚ â”‚ Duration: 2h 58m | Cost: $32.10 | Recall@100: 45.2%                   â”‚  â”‚
â”‚ â”‚ [View Results] [Rollback]                                              â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ âŒ Training Run #44                                           Failed   â”‚  â”‚
â”‚ â”‚ Dataset: Q4 2024 Training Data | Config: config-040                    â”‚  â”‚
â”‚ â”‚ Failed at: Transform | Error: OOM during vocabulary generation         â”‚  â”‚
â”‚ â”‚ [View Logs]                                                            â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### New Training Dialog

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Start Training Run                                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ SELECT CONFIGURATION                                                         â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Dataset *                                                                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Q4 2024 Training Data                                              [â–¼] â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â„¹ï¸ 2.45M rows | 98K users | 36K products | Last 6 months                    â”‚
â”‚                                                                              â”‚
â”‚ Feature Config *                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â˜… config-042: Large embeddings (Best: 47.3% R@100)                 [â–¼] â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â„¹ï¸ user_id: 64d | product_id: 64d | crosses: catÃ—subcat, userÃ—city          â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ TRAINING HYPERPARAMETERS                                                     â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Epochs:             [20 â–¼]        (recommended: 15-30)                       â”‚
â”‚ Batch Size:         [8192 â–¼]      (recommended: 4096-16384)                  â”‚
â”‚ Learning Rate:      [0.1 â–¼]       (Adagrad default)                          â”‚
â”‚ Early Stopping:     â˜‘ Enable      Patience: [5 â–¼] epochs                     â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ COMPUTE RESOURCES                                                            â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ GPU Configuration:  [4x T4 â–¼]     (options: 1x T4, 4x T4, 4x V100, 4x L4)    â”‚
â”‚ Use Preemptible:    â˜‘ Yes         (70% cost reduction, may be interrupted)  â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ ESTIMATES                                                                    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Estimated duration:  2-4 hours                                               â”‚
â”‚ Estimated cost:      $25-45 (with preemptible GPUs)                          â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                            [Cancel]  [â–¶ Start Training]     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Progress View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Run #47                                                   Running  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ Dataset: Q4 2024 Training Data                                               â”‚
â”‚ Feature Config: config-042 (Large embeddings)                                â”‚
â”‚ Started: Nov 28, 2024 14:30 | Elapsed: 1h 23m                               â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ PIPELINE PROGRESS                                                            â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 65%    â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚ âœ… ExampleGen         Completed     12 min     2,450,123 examples           â”‚
â”‚ âœ… StatisticsGen      Completed      8 min     Stats generated              â”‚
â”‚ âœ… SchemaGen          Completed     30 sec     Schema inferred              â”‚
â”‚ âœ… Transform          Completed     25 min     Vocabularies created         â”‚
â”‚ ðŸ”„ Trainer            Running       38 min     Epoch 8/20 (Loss: 0.31)      â”‚
â”‚ â³ Evaluator          Pending       -          -                             â”‚
â”‚ â³ Pusher             Pending       -          -                             â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ TRAINING METRICS (Live)                                                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Epoch    Loss      Recall@10   Recall@50   Recall@100                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚ 1        0.85      5.2%        12.1%       18.4%                            â”‚
â”‚ 2        0.62      8.7%        20.3%       29.5%                            â”‚
â”‚ 3        0.48      12.1%       28.5%       38.2%                            â”‚
â”‚ 4        0.41      14.8%       33.2%       42.1%                            â”‚
â”‚ 5        0.37      16.2%       35.8%       44.3%                            â”‚
â”‚ 6        0.34      17.1%       37.2%       45.6%                            â”‚
â”‚ 7        0.32      17.6%       38.1%       46.4%                            â”‚
â”‚ 8        0.31      17.9%       38.6%       46.8%      â† Current             â”‚
â”‚                                                                              â”‚
â”‚ [Training Curve Chart - Loss and Recall over epochs]                         â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ RESOURCE UTILIZATION                                                         â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ GPU Memory: 14.2 / 16.0 GB (89%)                                            â”‚
â”‚ GPU Utilization: 94%                                                         â”‚
â”‚ Current Cost: $18.42                                                         â”‚
â”‚                                                                              â”‚
â”‚                                                              [Cancel Run]    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Results View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Run #46 - Results                                       Completed  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ SUMMARY                                                                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Status:       âœ… Completed Successfully                                      â”‚
â”‚ Duration:     3h 42m                                                         â”‚
â”‚ Total Cost:   $38.50                                                         â”‚
â”‚ Early Stop:   Yes, at epoch 18 (patience: 5)                                â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ FINAL METRICS                                                                â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Metric         â”‚ Value      â”‚ vs Quick Test                          â”‚    â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚ â”‚ Loss           â”‚ 0.28       â”‚ â†“ 0.10 (was 0.38 in quick test)        â”‚    â”‚
â”‚ â”‚ Recall@10      â”‚ 18.9%      â”‚ â†‘ 0.7%                                 â”‚    â”‚
â”‚ â”‚ Recall@50      â”‚ 39.2%      â”‚ â†‘ 0.7%                                 â”‚    â”‚
â”‚ â”‚ Recall@100     â”‚ 46.8%      â”‚ â†“ 0.5% (quick test was optimistic)     â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ ARTIFACTS                                                                    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Model:         gs://b2b-recs-ml/models/run-46/saved_model/                  â”‚
â”‚ Query Model:   gs://b2b-recs-ml/models/run-46/query_model/                  â”‚
â”‚ Candidate Model: gs://b2b-recs-ml/models/run-46/candidate_model/            â”‚
â”‚ Transform:     gs://b2b-recs-ml/artifacts/run-46/transform/                 â”‚
â”‚ Vocabularies:  gs://b2b-recs-ml/artifacts/run-46/vocabularies/              â”‚
â”‚                                                                              â”‚
â”‚ ML Metadata:   [View in MLMD Console]                                       â”‚
â”‚ MLflow:        [View Experiment]                                            â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚ CONFIGURATION SNAPSHOT                                                       â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ Dataset:       Q4 2024 Training Data (version 3)                            â”‚
â”‚ Feature Config: config-042 (Large embeddings)                               â”‚
â”‚ Epochs:        18 (early stopped)                                           â”‚
â”‚ Batch Size:    8192                                                         â”‚
â”‚ Learning Rate: 0.1 (Adagrad)                                                â”‚
â”‚ GPU:           4x T4 (preemptible)                                          â”‚
â”‚                                                                              â”‚
â”‚ [View Full Config JSON]                                                      â”‚
â”‚                                                                              â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                              â”‚
â”‚ [View in MLflow]  [Compare with Other Runs]  [â–¶ Deploy This Model]          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Model

### Django Models

```python
# ml_platform/models.py

class TrainingRun(models.Model):
    """
    Tracks a full training pipeline execution.
    """
    # Basic info
    ml_model = models.ForeignKey('MLModel', on_delete=models.CASCADE, related_name='training_runs')
    run_number = models.IntegerField()  # Auto-incremented per model

    # Configuration links
    dataset = models.ForeignKey('Dataset', on_delete=models.PROTECT)
    dataset_version = models.ForeignKey('DatasetVersion', on_delete=models.PROTECT, null=True)
    feature_config = models.ForeignKey('FeatureConfig', on_delete=models.PROTECT)

    # Training hyperparameters (JSON)
    hyperparameters = models.JSONField(default=dict)
    # Example:
    # {
    #   "epochs": 20,
    #   "batch_size": 8192,
    #   "learning_rate": 0.1,
    #   "optimizer": "adagrad",
    #   "early_stopping": {"enabled": true, "patience": 5}
    # }

    # Compute configuration (JSON)
    compute_config = models.JSONField(default=dict)
    # Example:
    # {
    #   "gpu_type": "T4",
    #   "gpu_count": 4,
    #   "preemptible": true,
    #   "machine_type": "n1-standard-32"
    # }

    # Status
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('running', 'Running'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('cancelled', 'Cancelled'),
    ]
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    current_stage = models.CharField(max_length=100, blank=True)
    current_epoch = models.IntegerField(null=True, blank=True)
    total_epochs = models.IntegerField(null=True, blank=True)

    # Pipeline tracking
    vertex_pipeline_id = models.CharField(max_length=255, blank=True)
    vertex_pipeline_url = models.URLField(blank=True)

    # Results
    final_loss = models.FloatField(null=True, blank=True)
    recall_at_10 = models.FloatField(null=True, blank=True)
    recall_at_50 = models.FloatField(null=True, blank=True)
    recall_at_100 = models.FloatField(null=True, blank=True)
    early_stopped_at_epoch = models.IntegerField(null=True, blank=True)

    # Artifacts (JSON)
    artifacts = models.JSONField(default=dict)
    # Example:
    # {
    #   "saved_model": "gs://bucket/models/run-46/saved_model/",
    #   "query_model": "gs://bucket/models/run-46/query_model/",
    #   "candidate_model": "gs://bucket/models/run-46/candidate_model/",
    #   "transform": "gs://bucket/artifacts/run-46/transform/",
    #   "vocabularies": "gs://bucket/artifacts/run-46/vocabularies/"
    # }

    # Cost tracking
    cost_usd = models.DecimalField(max_digits=10, decimal_places=2, null=True, blank=True)
    duration_seconds = models.IntegerField(null=True, blank=True)

    # Error tracking
    error_message = models.TextField(blank=True)
    error_stage = models.CharField(max_length=100, blank=True)

    # ML Metadata & MLflow
    mlmd_context_id = models.CharField(max_length=255, blank=True)
    mlflow_run_id = models.CharField(max_length=255, blank=True)

    # Deployment status
    is_deployed = models.BooleanField(default=False)
    deployed_at = models.DateTimeField(null=True, blank=True)

    # Timestamps
    started_at = models.DateTimeField(null=True, blank=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    created_at = models.DateTimeField(auto_now_add=True)

    class Meta:
        ordering = ['-created_at']
        unique_together = ['ml_model', 'run_number']

    def save(self, *args, **kwargs):
        if not self.run_number:
            # Auto-increment run number for this model
            last_run = TrainingRun.objects.filter(ml_model=self.ml_model).order_by('-run_number').first()
            self.run_number = (last_run.run_number + 1) if last_run else 1
        super().save(*args, **kwargs)


class TrainingMetricsHistory(models.Model):
    """
    Stores per-epoch metrics for training visualization.
    """
    training_run = models.ForeignKey(TrainingRun, on_delete=models.CASCADE, related_name='metrics_history')
    epoch = models.IntegerField()
    loss = models.FloatField()
    recall_at_10 = models.FloatField(null=True, blank=True)
    recall_at_50 = models.FloatField(null=True, blank=True)
    recall_at_100 = models.FloatField(null=True, blank=True)
    timestamp = models.DateTimeField(auto_now_add=True)

    class Meta:
        ordering = ['epoch']
        unique_together = ['training_run', 'epoch']
```

---

## API Endpoints

### Training Run CRUD

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/models/{model_id}/training-runs/` | List training runs |
| POST | `/api/models/{model_id}/training-runs/` | Start new training run |
| GET | `/api/training-runs/{run_id}/` | Get training run details |
| POST | `/api/training-runs/{run_id}/cancel/` | Cancel running training |
| GET | `/api/training-runs/{run_id}/logs/` | Get training logs |
| GET | `/api/training-runs/{run_id}/metrics/` | Get metrics history |

### Webhooks (for pipeline callbacks)

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/training-runs/{run_id}/webhook/stage-complete/` | Pipeline stage completed |
| POST | `/api/training-runs/{run_id}/webhook/epoch-complete/` | Training epoch completed |
| POST | `/api/training-runs/{run_id}/webhook/failed/` | Pipeline failed |
| POST | `/api/training-runs/{run_id}/webhook/completed/` | Pipeline completed |

---

## TFX Pipeline

### Full Training Pipeline Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FULL TRAINING TFX PIPELINE                                â”‚
â”‚                                                                              â”‚
â”‚   BigQuery        ExampleGen        StatisticsGen        SchemaGen          â”‚
â”‚   (100% data)     (TFRecords)       (full stats)         (schema)           â”‚
â”‚       â”‚               â”‚                  â”‚                  â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                  â”‚                                           â”‚
â”‚                                  â†“                                           â”‚
â”‚                             Transform                                        â”‚
â”‚                         (full vocabularies)                                  â”‚
â”‚                                  â”‚                                           â”‚
â”‚                                  â†“                                           â”‚
â”‚                              Trainer                                         â”‚
â”‚                      (GPU, multi-epoch, early stopping)                      â”‚
â”‚                                  â”‚                                           â”‚
â”‚                                  â†“                                           â”‚
â”‚                             Evaluator                                        â”‚
â”‚                        (compute final metrics)                               â”‚
â”‚                                  â”‚                                           â”‚
â”‚                                  â†“                                           â”‚
â”‚                              Pusher                                          â”‚
â”‚                      (push to model registry)                                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â†“
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   ML Metadata   â”‚
                         â”‚   (artifact     â”‚
                         â”‚    tracking)    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vertex AI Pipelines Integration

```python
# ml_platform/training/services.py

from google.cloud import aiplatform
from tfx.orchestration import pipeline as tfx_pipeline
from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner

class TrainingPipelineService:
    """
    Manages TFX pipeline execution via Vertex AI Pipelines.
    """

    def __init__(self, project_id: str, region: str):
        self.project_id = project_id
        self.region = region
        aiplatform.init(project=project_id, location=region)

    def create_pipeline(
        self,
        training_run: 'TrainingRun',
        dataset: 'Dataset',
        feature_config: 'FeatureConfig'
    ) -> tfx_pipeline.Pipeline:
        """
        Create TFX Pipeline object from training configuration.
        """
        pass

    def compile_pipeline(self, pipeline: tfx_pipeline.Pipeline) -> str:
        """
        Compile pipeline to Kubeflow Pipelines IR.
        Returns path to compiled pipeline JSON.
        """
        pass

    def submit_pipeline(
        self,
        compiled_pipeline_path: str,
        training_run: 'TrainingRun'
    ) -> str:
        """
        Submit pipeline to Vertex AI Pipelines.
        Returns pipeline run ID.
        """
        pass

    def get_pipeline_status(self, pipeline_run_id: str) -> dict:
        """
        Get current status of a pipeline run.
        """
        pass

    def cancel_pipeline(self, pipeline_run_id: str) -> bool:
        """
        Cancel a running pipeline.
        """
        pass
```

### TFX Pipeline Definition

```python
# training/tfx_pipeline.py

from tfx import v1 as tfx
from tfx.components import (
    BigQueryExampleGen,
    StatisticsGen,
    SchemaGen,
    Transform,
    Trainer,
    Evaluator,
    Pusher,
)
from tfx.proto import trainer_pb2
from tfx.extensions.google_cloud_ai_platform.trainer import executor as ai_platform_trainer_executor

def create_tfrs_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    bigquery_query: str,
    preprocessing_fn_path: str,
    trainer_module_path: str,
    hyperparameters: dict,
    compute_config: dict,
    serving_model_dir: str,
) -> tfx.dsl.Pipeline:
    """
    Create a TFX pipeline for TFRS model training.
    """

    # ExampleGen - Extract data from BigQuery
    example_gen = BigQueryExampleGen(
        query=bigquery_query,
        output_config=tfx.proto.Output(
            split_config=tfx.proto.SplitConfig(
                splits=[
                    tfx.proto.SplitConfig.Split(name='train', hash_buckets=10),
                    tfx.proto.SplitConfig.Split(name='eval', hash_buckets=2),
                ]
            )
        )
    )

    # StatisticsGen - Generate statistics
    statistics_gen = StatisticsGen(
        examples=example_gen.outputs['examples']
    )

    # SchemaGen - Infer schema
    schema_gen = SchemaGen(
        statistics=statistics_gen.outputs['statistics']
    )

    # Transform - Feature preprocessing
    transform = Transform(
        examples=example_gen.outputs['examples'],
        schema=schema_gen.outputs['schema'],
        preprocessing_fn=preprocessing_fn_path,
    )

    # Trainer - Train TFRS model
    trainer = Trainer(
        module_file=trainer_module_path,
        examples=transform.outputs['transformed_examples'],
        transform_graph=transform.outputs['transform_graph'],
        schema=schema_gen.outputs['schema'],
        train_args=trainer_pb2.TrainArgs(num_steps=hyperparameters['train_steps']),
        eval_args=trainer_pb2.EvalArgs(num_steps=hyperparameters['eval_steps']),
        custom_config={
            'epochs': hyperparameters['epochs'],
            'batch_size': hyperparameters['batch_size'],
            'learning_rate': hyperparameters['learning_rate'],
            'early_stopping_patience': hyperparameters.get('early_stopping_patience', 5),
        },
        custom_executor_spec=tfx.dsl.executor_spec.ExecutorClassSpec(
            ai_platform_trainer_executor.GenericExecutor
        ),
    )

    # Evaluator - Evaluate model
    evaluator = Evaluator(
        examples=example_gen.outputs['examples'],
        model=trainer.outputs['model'],
    )

    # Pusher - Push model to serving
    pusher = Pusher(
        model=trainer.outputs['model'],
        push_destination=tfx.proto.PushDestination(
            filesystem=tfx.proto.PushDestination.Filesystem(
                base_directory=serving_model_dir
            )
        )
    )

    return tfx.dsl.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=[
            example_gen,
            statistics_gen,
            schema_gen,
            transform,
            trainer,
            evaluator,
            pusher,
        ],
    )
```

### TFRS Trainer Module

```python
# training/tfrs_trainer.py

import tensorflow as tf
import tensorflow_recommenders as tfrs
from tfx.components.trainer.fn_args_utils import FnArgs

def run_fn(fn_args: FnArgs):
    """
    TFX Trainer module entry point for TFRS model.
    """
    # Load hyperparameters from custom_config
    hyperparams = fn_args.custom_config

    # Create tf.data datasets from TFRecords
    train_dataset = _create_dataset(
        fn_args.train_files,
        fn_args.data_accessor,
        fn_args.schema,
        batch_size=hyperparams['batch_size'],
    )

    eval_dataset = _create_dataset(
        fn_args.eval_files,
        fn_args.data_accessor,
        fn_args.schema,
        batch_size=hyperparams['batch_size'],
    )

    # Load vocabularies from Transform output
    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)

    # Build TFRS model
    model = TFRSModel(
        tf_transform_output=tf_transform_output,
        hyperparams=hyperparams,
    )

    # Compile
    model.compile(optimizer=tf.keras.optimizers.Adagrad(hyperparams['learning_rate']))

    # Callbacks
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=hyperparams['early_stopping_patience'],
            restore_best_weights=True,
        ),
        # Custom callback to log metrics to Django
        TrainingMetricsCallback(fn_args.custom_config.get('webhook_url')),
    ]

    # Train
    model.fit(
        train_dataset,
        validation_data=eval_dataset,
        epochs=hyperparams['epochs'],
        callbacks=callbacks,
    )

    # Save model
    model.save(fn_args.serving_model_dir)


class TFRSModel(tfrs.Model):
    """
    Two-tower retrieval model for recommendations.
    """

    def __init__(self, tf_transform_output, hyperparams):
        super().__init__()
        self.query_model = self._build_query_tower(tf_transform_output, hyperparams)
        self.candidate_model = self._build_candidate_tower(tf_transform_output, hyperparams)
        self.task = tfrs.tasks.Retrieval()

    def _build_query_tower(self, tf_transform_output, hyperparams):
        """Build the query (user) tower."""
        # Implementation based on FeatureConfig
        pass

    def _build_candidate_tower(self, tf_transform_output, hyperparams):
        """Build the candidate (product) tower."""
        # Implementation based on FeatureConfig
        pass

    def compute_loss(self, features, training=False):
        query_embeddings = self.query_model(features)
        candidate_embeddings = self.candidate_model(features)
        return self.task(query_embeddings, candidate_embeddings)
```

---

## Webhook Integration

Django receives status updates from the running pipeline:

```python
# ml_platform/training/webhooks.py

@csrf_exempt
@require_POST
def stage_complete_webhook(request, run_id):
    """
    Called when a pipeline stage completes.
    """
    data = json.loads(request.body)
    training_run = get_object_or_404(TrainingRun, id=run_id)

    training_run.current_stage = data['stage']
    training_run.save()

    # Notify frontend via WebSocket or polling
    return JsonResponse({'status': 'ok'})


@csrf_exempt
@require_POST
def epoch_complete_webhook(request, run_id):
    """
    Called after each training epoch.
    """
    data = json.loads(request.body)
    training_run = get_object_or_404(TrainingRun, id=run_id)

    # Update current epoch
    training_run.current_epoch = data['epoch']
    training_run.save()

    # Save metrics history
    TrainingMetricsHistory.objects.create(
        training_run=training_run,
        epoch=data['epoch'],
        loss=data['loss'],
        recall_at_10=data.get('recall_at_10'),
        recall_at_50=data.get('recall_at_50'),
        recall_at_100=data.get('recall_at_100'),
    )

    return JsonResponse({'status': 'ok'})
```

---

## Implementation Checklist

### Phase 1: Basic Training Run
- [ ] Create Django models (TrainingRun, TrainingMetricsHistory)
- [ ] Create training sub-app structure
- [ ] Implement basic API endpoints
- [ ] Create training runs list page
- [ ] Create new training dialog

### Phase 2: TFX Pipeline Integration
- [ ] Create TFX pipeline definition
- [ ] Implement TFRS trainer module
- [ ] Compile pipeline to Kubeflow IR
- [ ] Submit to Vertex AI Pipelines

### Phase 3: Progress Tracking
- [ ] Implement webhook endpoints
- [ ] Create training progress view
- [ ] Real-time metrics display
- [ ] Training curve visualization

### Phase 4: Results & Artifacts
- [ ] Display final metrics
- [ ] Link to artifacts in GCS
- [ ] ML Metadata integration
- [ ] MLflow logging

---

## Dependencies on Other Domains

### Depends On
- **Datasets Domain**: Provides Dataset definition for ExampleGen
- **Modeling Domain**: Provides Feature Config for Transform

### Depended On By
- **Experiments Domain**: Training results feed into comparison
- **Deployment Domain**: Completed runs can be deployed

---

## Related Documentation

- [Implementation Overview](../implementation.md)
- [Datasets Phase](phase_datasets.md)
- [Modeling Phase](phase_modeling.md)
- [Experiments Phase](phase_experiments.md)
- [Deployment Phase](phase_deployment.md)
