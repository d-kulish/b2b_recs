#!/usr/bin/env python3
"""
Test Services Trainer Generation

Tests TrainerModuleGenerator from services.py by:
1. Loading configs from an existing experiment
2. Generating trainer_module.py using services.py
3. Running a CustomJob with that generated code

This tests the ACTUAL code generation path used by pipelines.

Usage:
    python scripts/test_services_trainer.py --experiment-id 62 --epochs 2
"""

import argparse
import logging
import os
import sys
from datetime import datetime

# Setup Django
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
os.chdir(project_root)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')

import django
django.setup()

from ml_platform.models import FeatureConfig, ModelConfig, Experiment
from ml_platform.configs.services import TrainerModuleGenerator, validate_python_code

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

PROJECT_ID = 'b2b-recs'
REGION = 'europe-central2'
STAGING_BUCKET = 'b2b-recs-pipeline-staging'
ARTIFACTS_BUCKET = 'b2b-recs-quicktest-artifacts'
TRAINER_IMAGE = 'europe-central2-docker.pkg.dev/b2b-recs/tfx-builder/tfx-trainer:latest'


def find_artifacts(source_exp: str) -> dict:
    """Find Transform artifacts from a completed experiment."""
    from google.cloud import storage

    client = storage.Client()
    bucket = client.bucket(STAGING_BUCKET)

    prefix = f'pipeline_root/{source_exp}/'
    blobs = list(bucket.list_blobs(prefix=prefix))
    paths = [b.name for b in blobs]

    transform_base = None
    schema_path = None

    for p in paths:
        if 'Transform_' in p and '/transform_graph/' in p and not transform_base:
            parts = p.split('/')
            for i, part in enumerate(parts):
                if part.startswith('Transform_'):
                    transform_base = '/'.join(parts[:i+1])
                    break
        if 'SchemaGen_' in p and 'schema.pbtxt' in p:
            schema_path = p

    if not transform_base or not schema_path:
        raise ValueError(f"Could not find artifacts in {prefix}")

    return {
        'transform_graph': f'gs://{STAGING_BUCKET}/{transform_base}/transform_graph',
        'transformed_examples_train': f'gs://{STAGING_BUCKET}/{transform_base}/transformed_examples/Split-train',
        'transformed_examples_eval': f'gs://{STAGING_BUCKET}/{transform_base}/transformed_examples/Split-eval',
        'schema': f'gs://{STAGING_BUCKET}/{schema_path}',
    }


def create_runner_script(artifacts: dict, trainer_gcs_path: str, output_path: str, gcs_output_path: str, epochs: int, learning_rate: float) -> str:
    """Create the script that will run inside the CustomJob."""

    return f'''#!/usr/bin/env python3
"""Trainer runner script for CustomJob - testing services.py generation."""
import os
import sys
import logging
import tempfile
import subprocess

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("="*60)
    logger.info("SERVICES.PY TRAINER TEST")
    logger.info("="*60)

    work_dir = tempfile.mkdtemp(prefix='trainer_test_')
    logger.info(f"Work dir: {{work_dir}}")

    # Download trainer module (generated by services.py)
    trainer_local = os.path.join(work_dir, 'trainer_module.py')
    logger.info(f"Downloading trainer from {trainer_gcs_path}")
    subprocess.run(['gsutil', 'cp', '{trainer_gcs_path}', trainer_local], check=True)

    # Download transform graph
    transform_local = os.path.join(work_dir, 'transform_graph')
    os.makedirs(transform_local, exist_ok=True)
    logger.info(f"Downloading transform graph from {artifacts['transform_graph']}")
    subprocess.run(['gsutil', '-m', 'rsync', '-r', '{artifacts['transform_graph']}', transform_local], check=True)

    # Download schema
    schema_local = os.path.join(work_dir, 'schema.pbtxt')
    logger.info(f"Downloading schema from {artifacts['schema']}")
    subprocess.run(['gsutil', 'cp', '{artifacts['schema']}', schema_local], check=True)

    # Import trainer module
    logger.info("Importing trainer module...")
    sys.path.insert(0, work_dir)
    import importlib.util
    spec = importlib.util.spec_from_file_location("trainer_module", trainer_local)
    trainer = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(trainer)

    # Create FnArgs
    class FnArgs:
        pass

    fn_args = FnArgs()
    fn_args.train_files = ['{artifacts['transformed_examples_train']}/*']
    fn_args.eval_files = ['{artifacts['transformed_examples_eval']}/*']
    fn_args.transform_output = transform_local
    fn_args.schema_file = schema_local
    fn_args.serving_model_dir = os.path.join(work_dir, 'serving_model')
    fn_args.model_run_dir = os.path.join(work_dir, 'model_run')
    fn_args.train_steps = None
    fn_args.eval_steps = None
    fn_args.custom_config = {{
        'epochs': {epochs},
        'batch_size': 4096,
        'learning_rate': {learning_rate},
        'gcs_output_path': '{gcs_output_path}',  # For MetricsCollector to save training_metrics.json
    }}

    # Create data accessor
    from tfx_bsl.public import tfxio

    class DataAccessor:
        def tf_dataset_factory(self, file_pattern, options, schema):
            import tensorflow as tf

            if isinstance(file_pattern, list):
                file_pattern = file_pattern[0]

            files = tf.io.gfile.glob(file_pattern)
            logger.info(f"Found {{len(files)}} files matching {{file_pattern}}")

            dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')

            import tensorflow_transform as tft
            tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)
            feature_spec = tf_transform_output.transformed_feature_spec()

            def parse_fn(example):
                return tf.io.parse_single_example(example, feature_spec)

            dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
            dataset = dataset.batch(options.batch_size)
            dataset = dataset.prefetch(tf.data.AUTOTUNE)
            return dataset

    fn_args.data_accessor = DataAccessor()

    os.makedirs(fn_args.serving_model_dir, exist_ok=True)
    os.makedirs(fn_args.model_run_dir, exist_ok=True)

    logger.info("="*60)
    logger.info("STARTING TRAINING")
    logger.info("="*60)

    trainer.run_fn(fn_args)

    logger.info("="*60)
    logger.info("TRAINING COMPLETED SUCCESSFULLY")
    logger.info("="*60)

    logger.info(f"Uploading model to {output_path}")
    subprocess.run(['gsutil', '-m', 'cp', '-r', fn_args.serving_model_dir, '{output_path}'], check=True)

if __name__ == '__main__':
    main()
'''


def main():
    parser = argparse.ArgumentParser(description='Test services.py trainer generation')
    parser.add_argument('--feature-config-id', type=int, default=5, help='FeatureConfig ID')
    parser.add_argument('--model-config-id', type=int, default=6, help='ModelConfig ID')
    parser.add_argument('--source-exp', default='qt-62-20251231-154907', help='Source experiment for artifacts')
    parser.add_argument('--epochs', type=int, default=2, help='Training epochs')
    parser.add_argument('--learning-rate', type=float, default=0.1, help='Learning rate')
    parser.add_argument('--dry-run', action='store_true', help='Generate code but do not submit job')

    args = parser.parse_args()

    from google.cloud import storage, aiplatform

    run_id = f'services-test-{datetime.now().strftime("%Y%m%d-%H%M%S")}'
    logger.info(f"Run ID: {run_id}")

    # Get configs from database
    logger.info(f"Loading FeatureConfig {args.feature_config_id} and ModelConfig {args.model_config_id}...")

    feature_config = FeatureConfig.objects.get(id=args.feature_config_id)
    model_config = ModelConfig.objects.get(id=args.model_config_id)

    logger.info(f"  FeatureConfig: {feature_config.name}")
    logger.info(f"  ModelConfig: {model_config.name}")
    logger.info(f"  Epochs: {args.epochs}")
    logger.info(f"  Learning Rate: {args.learning_rate}")

    # Generate trainer code using services.py
    logger.info("Generating trainer_module.py using TrainerModuleGenerator...")
    generator = TrainerModuleGenerator(feature_config, model_config)
    trainer_code = generator.generate()

    # Validate the generated code
    is_valid, error_msg, error_line = validate_python_code(trainer_code, 'trainer')
    if not is_valid:
        logger.error(f"Generated code is INVALID: {error_msg} at line {error_line}")
        # Save for debugging
        with open('/tmp/invalid_trainer.py', 'w') as f:
            f.write(trainer_code)
        logger.error("Saved invalid code to /tmp/invalid_trainer.py")
        sys.exit(1)

    logger.info("Generated code is syntactically valid")

    # Override epochs and learning rate in the generated code
    import re
    trainer_code = re.sub(r'EPOCHS = \d+', f'EPOCHS = {args.epochs}', trainer_code)
    trainer_code = re.sub(r'LEARNING_RATE = [\d.]+', f'LEARNING_RATE = {args.learning_rate}', trainer_code)

    # Find artifacts
    logger.info(f"Finding artifacts from {args.source_exp}...")
    artifacts = find_artifacts(args.source_exp)
    for k, v in artifacts.items():
        logger.info(f"  {k}: {v}")

    # Upload trainer
    client = storage.Client()
    bucket = client.bucket(ARTIFACTS_BUCKET)
    trainer_blob_path = f'{run_id}/trainer_module.py'
    bucket.blob(trainer_blob_path).upload_from_string(trainer_code)
    trainer_gcs_path = f'gs://{ARTIFACTS_BUCKET}/{trainer_blob_path}'
    logger.info(f"Uploaded trainer to {trainer_gcs_path}")

    # Create runner script
    gcs_output_path = f'gs://{ARTIFACTS_BUCKET}/{run_id}'  # Base path for MetricsCollector
    output_path = f'{gcs_output_path}/model'  # Model output subdirectory
    runner_script = create_runner_script(artifacts, trainer_gcs_path, output_path, gcs_output_path, args.epochs, args.learning_rate)

    # Upload runner script
    runner_blob_path = f'{run_id}/runner.py'
    bucket.blob(runner_blob_path).upload_from_string(runner_script)
    runner_gcs_path = f'gs://{ARTIFACTS_BUCKET}/{runner_blob_path}'
    logger.info(f"Uploaded runner to {runner_gcs_path}")

    if args.dry_run:
        logger.info("DRY RUN - not submitting")
        logger.info(f"Generated trainer saved to: {trainer_gcs_path}")
        logger.info(f"You can inspect it with: gsutil cat {trainer_gcs_path}")
        return

    # Submit CustomJob
    aiplatform.init(project=PROJECT_ID, location=REGION)

    job = aiplatform.CustomJob(
        display_name=f'services-test-{run_id}',
        worker_pool_specs=[{
            'machine_spec': {'machine_type': 'n1-standard-4'},
            'replica_count': 1,
            'container_spec': {
                'image_uri': TRAINER_IMAGE,
                'command': ['bash', '-c', f'gsutil cp {runner_gcs_path} /tmp/runner.py && python /tmp/runner.py'],
            },
        }],
        staging_bucket=f'gs://{STAGING_BUCKET}',
    )

    logger.info("Submitting CustomJob...")
    job.submit()

    logger.info(f"""
================================================================================
CUSTOM JOB SUBMITTED - Testing services.py TrainerModuleGenerator
================================================================================
Run ID: {run_id}
Job: {job.resource_name}
Epochs: {args.epochs}

Monitor:
  gcloud ai custom-jobs describe {job.resource_name.split('/')[-1]} --region={REGION}

Logs:
  https://console.cloud.google.com/logs/query?project={PROJECT_ID}

Generated trainer:
  gsutil cat {trainer_gcs_path}

Output:
  Model: {output_path}
  Metrics: {gcs_output_path}/training_metrics.json

Verify metrics after completion:
  gsutil cat {gcs_output_path}/training_metrics.json | python -m json.tool | head -50
================================================================================
""")


if __name__ == '__main__':
    main()
