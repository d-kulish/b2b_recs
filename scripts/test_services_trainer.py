#!/usr/bin/env python3
"""
Test Services Trainer Generation

Tests TrainerModuleGenerator from services.py by:
1. Loading configs from an existing experiment
2. Generating trainer_module.py using services.py
3. Running a CustomJob with that generated code

This tests the ACTUAL code generation path used by pipelines.

Usage:
    python scripts/test_services_trainer.py --experiment-id 62 --epochs 2
"""

import argparse
import logging
import os
import sys
from datetime import datetime

# Setup Django
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
os.chdir(project_root)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')

import django
django.setup()

from ml_platform.models import FeatureConfig, ModelConfig, QuickTest
from ml_platform.configs.services import TrainerModuleGenerator, validate_python_code

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

PROJECT_ID = 'b2b-recs'
REGION = 'europe-west4'  # GPU training region (europe-central2 has no GPU support)
STAGING_BUCKET = 'b2b-recs-pipeline-staging'  # For reading artifacts (europe-central2)
JOB_STAGING_BUCKET = 'b2b-recs-gpu-staging'  # For job staging (must match REGION)
ARTIFACTS_BUCKET = 'b2b-recs-quicktest-artifacts'
TRAINER_IMAGE = 'europe-central2-docker.pkg.dev/b2b-recs/tfx-builder/tfx-trainer-gpu:latest'


def find_artifacts(source_exp: str) -> dict:
    """Find Transform artifacts from a completed experiment."""
    from google.cloud import storage

    client = storage.Client()
    bucket = client.bucket(STAGING_BUCKET)

    prefix = f'pipeline_root/{source_exp}/'
    blobs = list(bucket.list_blobs(prefix=prefix))
    paths = [b.name for b in blobs]

    transform_base = None
    schema_path = None

    for p in paths:
        if 'Transform_' in p and '/transform_graph/' in p and not transform_base:
            parts = p.split('/')
            for i, part in enumerate(parts):
                if part.startswith('Transform_'):
                    transform_base = '/'.join(parts[:i+1])
                    break
        if 'SchemaGen_' in p and 'schema.pbtxt' in p:
            schema_path = p

    if not transform_base or not schema_path:
        raise ValueError(f"Could not find artifacts in {prefix}")

    return {
        'transform_graph': f'gs://{STAGING_BUCKET}/{transform_base}/transform_graph',
        'transformed_examples_train': f'gs://{STAGING_BUCKET}/{transform_base}/transformed_examples/Split-train',
        'transformed_examples_eval': f'gs://{STAGING_BUCKET}/{transform_base}/transformed_examples/Split-eval',
        'schema': f'gs://{STAGING_BUCKET}/{schema_path}',
    }


def create_runner_script(artifacts: dict, trainer_gcs_path: str, output_path: str, gcs_output_path: str, epochs: int, learning_rate: float) -> str:
    """Create the script that will run inside the CustomJob."""

    return f'''#!/usr/bin/env python3
"""Trainer runner script for CustomJob - testing services.py generation."""
import os
import sys
import logging
import tempfile
import subprocess

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("="*60)
    logger.info("SERVICES.PY TRAINER TEST")
    logger.info("="*60)

    work_dir = tempfile.mkdtemp(prefix='trainer_test_')
    logger.info(f"Work dir: {{work_dir}}")

    # Download trainer module (generated by services.py)
    trainer_local = os.path.join(work_dir, 'trainer_module.py')
    logger.info(f"Downloading trainer from {trainer_gcs_path}")
    subprocess.run(['gsutil', 'cp', '{trainer_gcs_path}', trainer_local], check=True)

    # Download transform graph
    transform_local = os.path.join(work_dir, 'transform_graph')
    os.makedirs(transform_local, exist_ok=True)
    logger.info(f"Downloading transform graph from {artifacts['transform_graph']}")
    subprocess.run(['gsutil', '-m', 'rsync', '-r', '{artifacts['transform_graph']}', transform_local], check=True)

    # Download schema
    schema_local = os.path.join(work_dir, 'schema.pbtxt')
    logger.info(f"Downloading schema from {artifacts['schema']}")
    subprocess.run(['gsutil', 'cp', '{artifacts['schema']}', schema_local], check=True)

    # Import trainer module
    logger.info("Importing trainer module...")
    sys.path.insert(0, work_dir)
    import importlib.util
    spec = importlib.util.spec_from_file_location("trainer_module", trainer_local)
    trainer = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(trainer)

    # Create FnArgs
    class FnArgs:
        pass

    fn_args = FnArgs()
    fn_args.train_files = ['{artifacts['transformed_examples_train']}/*']
    fn_args.eval_files = ['{artifacts['transformed_examples_eval']}/*']
    fn_args.transform_output = transform_local
    fn_args.schema_file = schema_local
    fn_args.serving_model_dir = os.path.join(work_dir, 'serving_model')
    fn_args.model_run_dir = os.path.join(work_dir, 'model_run')
    fn_args.train_steps = None
    fn_args.eval_steps = None
    fn_args.custom_config = {{
        'epochs': {epochs},
        'batch_size': 4096,
        'learning_rate': {learning_rate},
        'gcs_output_path': '{gcs_output_path}',  # For MetricsCollector to save training_metrics.json
        'gpu_enabled': True,
        'gpu_count': 1,
    }}

    # Create data accessor
    from tfx_bsl.public import tfxio

    class DataAccessor:
        def tf_dataset_factory(self, file_pattern, options, schema):
            import tensorflow as tf

            if isinstance(file_pattern, list):
                file_pattern = file_pattern[0]

            files = tf.io.gfile.glob(file_pattern)
            logger.info(f"Found {{len(files)}} files matching {{file_pattern}}")

            dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')

            import tensorflow_transform as tft
            tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)
            feature_spec = tf_transform_output.transformed_feature_spec()

            def parse_fn(example):
                parsed = tf.io.parse_single_example(example, feature_spec)
                # Expand dims for scalar features to match TFT expectations (batch, 1)
                expanded = {{}}
                for key, value in parsed.items():
                    if len(value.shape) == 0:  # Scalar
                        expanded[key] = tf.expand_dims(value, 0)
                    else:
                        expanded[key] = value
                return expanded

            dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
            dataset = dataset.batch(options.batch_size)

            # Handle label_key for ranking models - extract label from features
            label_key = getattr(options, 'label_key', None)
            if label_key:
                logger.info(f"Ranking model: extracting label_key='{{label_key}}' from features")
                def extract_label(features):
                    label = features.pop(label_key)
                    return features, label
                dataset = dataset.map(extract_label, num_parallel_calls=tf.data.AUTOTUNE)

            dataset = dataset.prefetch(tf.data.AUTOTUNE)
            return dataset

    fn_args.data_accessor = DataAccessor()

    os.makedirs(fn_args.serving_model_dir, exist_ok=True)
    os.makedirs(fn_args.model_run_dir, exist_ok=True)

    logger.info("="*60)
    logger.info("STARTING TRAINING")
    logger.info("="*60)

    trainer.run_fn(fn_args)

    logger.info("="*60)
    logger.info("TRAINING COMPLETED SUCCESSFULLY")
    logger.info("="*60)

    logger.info(f"Uploading model to {output_path}")
    subprocess.run(['gsutil', '-m', 'cp', '-r', fn_args.serving_model_dir, '{output_path}'], check=True)

if __name__ == '__main__':
    main()
'''


def create_quicktest_record(feature_config, model_config, run_id: str, epochs: int, learning_rate: float) -> 'QuickTest':
    """Create a QuickTest record to track this test run."""
    from django.utils import timezone

    # Get next experiment number
    last_qt = QuickTest.objects.order_by('-experiment_number').first()
    next_exp_num = (last_qt.experiment_number or 0) + 1 if last_qt else 1

    qt = QuickTest.objects.create(
        feature_config=feature_config,
        model_config=model_config,
        experiment_name=f'Multitask Test {run_id}',
        experiment_description=f'Custom job test for multitask trainer (FC#{feature_config.id}, MC#{model_config.id})',
        experiment_number=next_exp_num,
        status='running',
        data_sample_percent=100,
        split_strategy='random',
        epochs=epochs,
        batch_size=4096,
        learning_rate=learning_rate,
        gcs_artifacts_path=f'gs://{ARTIFACTS_BUCKET}/{run_id}',
        submitted_at=timezone.now(),
        started_at=timezone.now(),
    )
    logger.info(f"Created QuickTest record: ID={qt.id}, Exp#{qt.experiment_number}")
    return qt


def wait_for_job_completion(job, timeout_minutes: int = 30) -> str:
    """Wait for job to complete and return final state."""
    import time
    from google.cloud import aiplatform

    logger.info(f"Waiting for job completion (timeout: {timeout_minutes} min)...")
    start_time = time.time()
    timeout_seconds = timeout_minutes * 60
    job_name = job.resource_name

    while True:
        # Re-fetch job to get latest state
        current_job = aiplatform.CustomJob.get(job_name)
        state = current_job.state.name
        elapsed = int(time.time() - start_time)

        if state in ['JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED']:
            logger.info(f"Job finished with state: {state} (after {elapsed}s)")
            return state

        if elapsed > timeout_seconds:
            logger.error(f"Job timed out after {timeout_minutes} minutes")
            return 'TIMEOUT'

        logger.info(f"Job state: {state} (elapsed: {elapsed}s)")
        time.sleep(30)  # Poll every 30 seconds


def fetch_metrics_from_gcs(gcs_output_path: str) -> dict:
    """Fetch training_metrics.json from GCS and extract final metrics."""
    from google.cloud import storage
    import json

    # Parse bucket and path
    path = gcs_output_path.replace('gs://', '')
    bucket_name = path.split('/')[0]
    blob_path = '/'.join(path.split('/')[1:]) + '/training_metrics.json'

    logger.info(f"Fetching metrics from gs://{bucket_name}/{blob_path}")

    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_path)

    if not blob.exists():
        logger.warning(f"Metrics file not found: {blob_path}")
        return {}

    content = blob.download_as_string()
    metrics_data = json.loads(content)

    # Extract final metrics from the structure
    final_metrics = metrics_data.get('final_metrics', {})
    logger.info(f"Retrieved final metrics: {final_metrics}")

    return final_metrics


def update_quicktest_with_metrics(qt: 'QuickTest', metrics: dict, job_state: str):
    """Update QuickTest record with metrics from completed job."""
    from django.utils import timezone

    # Map job state to QuickTest status
    status_map = {
        'JOB_STATE_SUCCEEDED': 'completed',
        'JOB_STATE_FAILED': 'failed',
        'JOB_STATE_CANCELLED': 'cancelled',
        'TIMEOUT': 'failed',
    }

    qt.status = status_map.get(job_state, 'failed')
    qt.completed_at = timezone.now()

    if qt.status == 'completed' and metrics:
        # Retrieval metrics
        qt.recall_at_5 = metrics.get('recall_at_5')
        qt.recall_at_10 = metrics.get('recall_at_10')
        qt.recall_at_50 = metrics.get('recall_at_50')
        qt.recall_at_100 = metrics.get('recall_at_100')

        # Ranking metrics
        qt.rmse = metrics.get('rmse') or metrics.get('val_rmse')
        qt.mae = metrics.get('mae') or metrics.get('val_mae')
        qt.test_rmse = metrics.get('test_rmse')
        qt.test_mae = metrics.get('test_mae')

        # Loss
        qt.loss = metrics.get('final_loss') or metrics.get('loss')

        logger.info(f"Metrics saved to QuickTest #{qt.experiment_number}:")
        logger.info(f"  Recall@5:  {qt.recall_at_5}")
        logger.info(f"  Recall@10: {qt.recall_at_10}")
        logger.info(f"  Recall@50: {qt.recall_at_50}")
        logger.info(f"  Recall@100: {qt.recall_at_100}")
        logger.info(f"  RMSE: {qt.rmse}")
        logger.info(f"  MAE: {qt.mae}")
        logger.info(f"  Test RMSE: {qt.test_rmse}")
        logger.info(f"  Test MAE: {qt.test_mae}")
    elif qt.status == 'failed':
        qt.error_message = f'Job failed with state: {job_state}'
        logger.error(f"Job failed: {job_state}")

    qt.save()
    logger.info(f"QuickTest #{qt.experiment_number} updated with status: {qt.status}")


def main():
    parser = argparse.ArgumentParser(description='Test services.py trainer generation')
    parser.add_argument('--feature-config-id', type=int, default=5, help='FeatureConfig ID')
    parser.add_argument('--model-config-id', type=int, default=6, help='ModelConfig ID')
    parser.add_argument('--source-exp', default='qt-62-20251231-154907', help='Source experiment for artifacts')
    parser.add_argument('--epochs', type=int, default=2, help='Training epochs')
    parser.add_argument('--learning-rate', type=float, default=0.1, help='Learning rate')
    parser.add_argument('--output-gcs-path', type=str, default=None, help='Custom GCS output path (e.g., gs://bucket/qt-94-xxx)')
    parser.add_argument('--dry-run', action='store_true', help='Generate code but do not submit job')
    parser.add_argument('--create-quicktest', action='store_true', help='Create a QuickTest record in Django DB')
    parser.add_argument('--wait', action='store_true', help='Wait for job completion and fetch metrics')
    parser.add_argument('--timeout', type=int, default=30, help='Timeout in minutes for --wait (default: 30)')

    args = parser.parse_args()

    from google.cloud import storage, aiplatform

    # Use custom output path if provided, otherwise generate new run_id
    if args.output_gcs_path:
        run_id = args.output_gcs_path.split('/')[-1]  # Extract last part as run_id
        gcs_output_path = args.output_gcs_path
    else:
        run_id = f'services-test-{datetime.now().strftime("%Y%m%d-%H%M%S")}'
        gcs_output_path = None  # Will be set later
    logger.info(f"Run ID: {run_id}")

    # Get configs from database
    logger.info(f"Loading FeatureConfig {args.feature_config_id} and ModelConfig {args.model_config_id}...")

    feature_config = FeatureConfig.objects.get(id=args.feature_config_id)
    model_config = ModelConfig.objects.get(id=args.model_config_id)

    logger.info(f"  FeatureConfig: {feature_config.name}")
    logger.info(f"  ModelConfig: {model_config.name}")
    logger.info(f"  Epochs: {args.epochs}")
    logger.info(f"  Learning Rate: {args.learning_rate}")

    # Generate trainer code using services.py
    logger.info("Generating trainer_module.py using TrainerModuleGenerator...")
    generator = TrainerModuleGenerator(feature_config, model_config)
    trainer_code = generator.generate()

    # Validate the generated code
    is_valid, error_msg, error_line = validate_python_code(trainer_code, 'trainer')
    if not is_valid:
        logger.error(f"Generated code is INVALID: {error_msg} at line {error_line}")
        # Save for debugging
        with open('/tmp/invalid_trainer.py', 'w') as f:
            f.write(trainer_code)
        logger.error("Saved invalid code to /tmp/invalid_trainer.py")
        sys.exit(1)

    logger.info("Generated code is syntactically valid")

    # Override epochs and learning rate in the generated code
    import re
    trainer_code = re.sub(r'EPOCHS = \d+', f'EPOCHS = {args.epochs}', trainer_code)
    trainer_code = re.sub(r'LEARNING_RATE = [\d.]+', f'LEARNING_RATE = {args.learning_rate}', trainer_code)

    # Find artifacts
    logger.info(f"Finding artifacts from {args.source_exp}...")
    artifacts = find_artifacts(args.source_exp)
    for k, v in artifacts.items():
        logger.info(f"  {k}: {v}")

    # Upload trainer
    client = storage.Client()
    bucket = client.bucket(ARTIFACTS_BUCKET)
    trainer_blob_path = f'{run_id}/trainer_module.py'
    bucket.blob(trainer_blob_path).upload_from_string(trainer_code)
    trainer_gcs_path = f'gs://{ARTIFACTS_BUCKET}/{trainer_blob_path}'
    logger.info(f"Uploaded trainer to {trainer_gcs_path}")

    # Create runner script
    if not gcs_output_path:
        gcs_output_path = f'gs://{ARTIFACTS_BUCKET}/{run_id}'  # Base path for MetricsCollector
    output_path = f'{gcs_output_path}/model'  # Model output subdirectory
    logger.info(f"GCS output path: {gcs_output_path}")
    runner_script = create_runner_script(artifacts, trainer_gcs_path, output_path, gcs_output_path, args.epochs, args.learning_rate)

    # Upload runner script
    runner_blob_path = f'{run_id}/runner.py'
    bucket.blob(runner_blob_path).upload_from_string(runner_script)
    runner_gcs_path = f'gs://{ARTIFACTS_BUCKET}/{runner_blob_path}'
    logger.info(f"Uploaded runner to {runner_gcs_path}")

    if args.dry_run:
        logger.info("DRY RUN - not submitting")
        logger.info(f"Generated trainer saved to: {trainer_gcs_path}")
        logger.info(f"You can inspect it with: gsutil cat {trainer_gcs_path}")
        return

    # Create QuickTest record if requested
    quicktest = None
    if args.create_quicktest:
        quicktest = create_quicktest_record(
            feature_config, model_config, run_id, args.epochs, args.learning_rate
        )

    # Submit CustomJob
    aiplatform.init(project=PROJECT_ID, location=REGION)

    job = aiplatform.CustomJob(
        display_name=f'services-test-{run_id}',
        worker_pool_specs=[{
            'machine_spec': {
                'machine_type': 'n1-standard-8',
                'accelerator_type': 'NVIDIA_TESLA_T4',
                'accelerator_count': 1,
            },
            'replica_count': 1,
            'container_spec': {
                'image_uri': TRAINER_IMAGE,
                'command': ['bash', '-c', f'gsutil cp {runner_gcs_path} /tmp/runner.py && python /tmp/runner.py'],
            },
        }],
        staging_bucket=f'gs://{JOB_STAGING_BUCKET}',
    )

    logger.info("Submitting CustomJob...")
    job.submit()

    # Update QuickTest with job info
    if quicktest:
        quicktest.vertex_pipeline_job_name = job.resource_name
        quicktest.save()

    logger.info(f"""
================================================================================
CUSTOM JOB SUBMITTED - Testing services.py TrainerModuleGenerator
================================================================================
Run ID: {run_id}
Job: {job.resource_name}
Epochs: {args.epochs}
QuickTest: {'#' + str(quicktest.experiment_number) if quicktest else 'Not created (use --create-quicktest)'}

Monitor:
  gcloud ai custom-jobs describe {job.resource_name.split('/')[-1]} --region={REGION}

Logs:
  https://console.cloud.google.com/logs/query?project={PROJECT_ID}

Generated trainer:
  gsutil cat {trainer_gcs_path}

Output:
  Model: {output_path}
  Metrics: {gcs_output_path}/training_metrics.json

Verify metrics after completion:
  gsutil cat {gcs_output_path}/training_metrics.json | python -m json.tool | head -50
================================================================================
""")

    # Wait for completion and fetch metrics if requested
    if args.wait:
        job_state = wait_for_job_completion(job, timeout_minutes=args.timeout)

        if job_state == 'JOB_STATE_SUCCEEDED':
            # Fetch metrics from GCS
            metrics = fetch_metrics_from_gcs(gcs_output_path)

            if quicktest:
                update_quicktest_with_metrics(quicktest, metrics, job_state)

            # Print final summary
            logger.info(f"""
================================================================================
JOB COMPLETED SUCCESSFULLY
================================================================================
Final Metrics:
  Recall@5:   {metrics.get('recall_at_5', 'N/A')}
  Recall@10:  {metrics.get('recall_at_10', 'N/A')}
  Recall@50:  {metrics.get('recall_at_50', 'N/A')}
  Recall@100: {metrics.get('recall_at_100', 'N/A')}
  RMSE:       {metrics.get('rmse', metrics.get('val_rmse', 'N/A'))}
  MAE:        {metrics.get('mae', metrics.get('val_mae', 'N/A'))}
  Test RMSE:  {metrics.get('test_rmse', 'N/A')}
  Test MAE:   {metrics.get('test_mae', 'N/A')}

QuickTest: {'#' + str(quicktest.experiment_number) + ' (ID: ' + str(quicktest.id) + ')' if quicktest else 'Not created'}

Verify in Django:
  python -c "from ml_platform.models import QuickTest; qt=QuickTest.objects.get(id={quicktest.id if quicktest else 0}); print(f'recall_at_100={{qt.recall_at_100}}, rmse={{qt.rmse}}')"
================================================================================
""")
        else:
            if quicktest:
                update_quicktest_with_metrics(quicktest, {}, job_state)
            logger.error(f"Job failed with state: {job_state}")


if __name__ == '__main__':
    main()
