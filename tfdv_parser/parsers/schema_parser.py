"""
Schema Parser for TFDV Artifacts

Parses schema.pbtxt files generated by SchemaGen component
and returns schema information including:
- Feature names and types
- Presence constraints (required/optional)
- Domain constraints (min/max, vocabulary)
- Value constraints
"""
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict

from google.cloud import storage
from google.protobuf import text_format
from tensorflow_metadata.proto.v0 import schema_pb2

logger = logging.getLogger(__name__)


@dataclass
class FeatureSchema:
    """Schema information for a single feature."""
    name: str
    feature_type: str  # 'INT', 'FLOAT', 'STRING', 'BYTES', 'STRUCT'
    value_type: str  # 'INT64', 'FLOAT', 'BYTES', etc.
    presence: str  # 'required', 'optional'
    min_fraction: Optional[float]
    shape: Optional[str]  # 'scalar', 'fixed', 'variable'
    domain: Optional[str]  # Domain name if any
    constraints: Dict[str, Any]  # Additional constraints


class SchemaParser:
    """
    Parser for TensorFlow Metadata schema files.

    Reads schema.pbtxt from GCS and extracts comprehensive schema information.
    """

    def __init__(self, project_id: str = None):
        self.project_id = project_id
        self._storage_client = None

    @property
    def storage_client(self):
        if self._storage_client is None:
            self._storage_client = storage.Client(project=self.project_id)
        return self._storage_client

    def parse_from_gcs(self, gcs_path: str) -> Dict:
        """
        Parse schema from a GCS path.

        Args:
            gcs_path: Full GCS path to schema.pbtxt or pipeline_root

        Returns:
            Dict with parsed schema
        """
        try:
            # If path is pipeline_root, find the schema file
            if not gcs_path.endswith('.pbtxt'):
                schema_path = self._find_schema_file(gcs_path)
                if not schema_path:
                    return {
                        'available': False,
                        'error': 'Schema file not found in pipeline artifacts'
                    }
            else:
                schema_path = gcs_path

            # Download and parse the proto
            schema = self._load_schema_proto(schema_path)
            if not schema:
                return {
                    'available': False,
                    'error': 'Failed to parse schema file'
                }

            # Extract schema information
            return self._extract_schema(schema)

        except Exception as e:
            logger.exception(f"Error parsing schema: {e}")
            return {
                'available': False,
                'error': str(e)
            }

    def _find_schema_file(self, pipeline_root: str) -> Optional[str]:
        """Find schema.pbtxt file in pipeline artifacts."""
        # Parse bucket and prefix
        path = pipeline_root.replace('gs://', '')
        parts = path.split('/', 1)
        bucket_name = parts[0]
        prefix = parts[1] if len(parts) > 1 else ''

        bucket = self.storage_client.bucket(bucket_name)
        blobs = list(bucket.list_blobs(prefix=prefix, max_results=500))

        # Look for SchemaGen output
        for blob in blobs:
            if ('SchemaGen' in blob.name and
                'schema' in blob.name and
                blob.name.endswith('schema.pbtxt')):
                return f"gs://{bucket_name}/{blob.name}"

        # Fallback: any schema.pbtxt
        for blob in blobs:
            if blob.name.endswith('schema.pbtxt'):
                return f"gs://{bucket_name}/{blob.name}"

        return None

    def _load_schema_proto(self, gcs_path: str) -> Optional[schema_pb2.Schema]:
        """Load and parse schema proto from GCS."""
        path = gcs_path.replace('gs://', '')
        parts = path.split('/', 1)
        bucket_name = parts[0]
        blob_name = parts[1]

        bucket = self.storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        if not blob.exists():
            logger.warning(f"Schema blob not found: {gcs_path}")
            return None

        content = blob.download_as_text()
        schema = schema_pb2.Schema()
        text_format.Parse(content, schema)

        logger.info(f"Loaded schema with {len(schema.feature)} features")
        return schema

    def _extract_schema(self, schema: schema_pb2.Schema) -> Dict:
        """Extract schema information from proto."""
        features = []
        int_features = 0
        float_features = 0
        string_features = 0
        required_features = 0

        # Type mapping
        type_map = {
            schema_pb2.INT: ('INT', 'INT64'),
            schema_pb2.FLOAT: ('FLOAT', 'FLOAT'),
            schema_pb2.BYTES: ('STRING', 'BYTES'),
        }

        for feature in schema.feature:
            feature_schema = self._parse_feature_schema(feature, type_map)
            features.append(asdict(feature_schema))

            # Count by type
            if feature_schema.feature_type == 'INT':
                int_features += 1
            elif feature_schema.feature_type == 'FLOAT':
                float_features += 1
            else:
                string_features += 1

            if feature_schema.presence == 'required':
                required_features += 1

        # Extract string domains
        string_domains = {}
        for domain in schema.string_domain:
            string_domains[domain.name] = {
                'name': domain.name,
                'values': list(domain.value)[:20],  # Limit to 20 values
                'total_values': len(domain.value)
            }

        # Extract int domains
        int_domains = {}
        for domain in schema.int_domain:
            int_domains[domain.name] = {
                'name': domain.name,
                'min': domain.min if domain.HasField('min') else None,
                'max': domain.max if domain.HasField('max') else None,
                'is_categorical': domain.is_categorical if hasattr(domain, 'is_categorical') else False
            }

        # Extract float domains
        float_domains = {}
        for domain in schema.float_domain:
            float_domains[domain.name] = {
                'name': domain.name,
                'min': domain.min if domain.HasField('min') else None,
                'max': domain.max if domain.HasField('max') else None,
            }

        return {
            'available': True,
            'num_features': len(features),
            'num_int_features': int_features,
            'num_float_features': float_features,
            'num_string_features': string_features,
            'num_required_features': required_features,
            'features': features,
            'string_domains': string_domains,
            'int_domains': int_domains,
            'float_domains': float_domains,
        }

    def _parse_feature_schema(self, feature, type_map: Dict) -> FeatureSchema:
        """Parse schema for a single feature."""
        name = feature.name

        # Get type
        feature_type = 'UNKNOWN'
        value_type = 'UNKNOWN'
        if feature.type in type_map:
            feature_type, value_type = type_map[feature.type]

        # Get presence constraint
        presence = 'optional'
        min_fraction = None
        try:
            if feature.HasField('presence'):
                min_fraction = feature.presence.min_fraction
                if min_fraction >= 1.0:
                    presence = 'required'
                elif min_fraction > 0:
                    presence = f'optional ({min_fraction:.0%} expected)'
        except (ValueError, AttributeError):
            pass

        # Get shape info
        shape = None
        try:
            if feature.HasField('shape'):
                dims = feature.shape.dim
                if len(dims) == 0:
                    shape = 'scalar'
                elif all(d.size > 0 for d in dims):
                    shape = f'fixed{[d.size for d in dims]}'
                else:
                    shape = 'variable'
            elif feature.HasField('value_count'):
                vc = feature.value_count
                if vc.min == vc.max:
                    shape = f'fixed[{vc.min}]'
                else:
                    shape = f'variable[{vc.min}-{vc.max}]'
        except (ValueError, AttributeError):
            pass

        # Get domain
        domain = None
        constraints = {}

        try:
            if feature.HasField('int_domain'):
                domain = feature.int_domain.name if feature.int_domain.name else None
                if feature.int_domain.HasField('min'):
                    constraints['min'] = feature.int_domain.min
                if feature.int_domain.HasField('max'):
                    constraints['max'] = feature.int_domain.max
        except (ValueError, AttributeError):
            pass

        try:
            if feature.HasField('float_domain'):
                domain = feature.float_domain.name if feature.float_domain.name else None
                if feature.float_domain.HasField('min'):
                    constraints['min'] = feature.float_domain.min
                if feature.float_domain.HasField('max'):
                    constraints['max'] = feature.float_domain.max
        except (ValueError, AttributeError):
            pass

        try:
            if feature.HasField('string_domain'):
                domain = feature.string_domain.name if feature.string_domain.name else None
        except (ValueError, AttributeError):
            pass

        return FeatureSchema(
            name=name,
            feature_type=feature_type,
            value_type=value_type,
            presence=presence,
            min_fraction=min_fraction,
            shape=shape,
            domain=domain,
            constraints=constraints
        )
