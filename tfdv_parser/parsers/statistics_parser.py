"""
Statistics Parser for TFDV Artifacts

Parses FeatureStats.pb files generated by StatisticsGen component
and returns rich statistics matching the standard TFDV visualization:

Numeric Features:
- count, missing%, mean, std_dev, zeros%, min, median, max
- histogram data for visualization

Categorical Features:
- count, missing%, unique
- top values with frequencies
- value distribution for bar charts
"""
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import math

from google.cloud import storage
import tensorflow_data_validation as tfdv
from tensorflow_metadata.proto.v0 import statistics_pb2

logger = logging.getLogger(__name__)


@dataclass
class NumericStats:
    """Statistics for numeric features."""
    count: int
    missing: int
    missing_pct: float
    mean: Optional[float]
    std_dev: Optional[float]
    zeros: int
    zeros_pct: float
    min_val: Optional[float]
    median: Optional[float]
    max_val: Optional[float]
    histogram: Optional[List[Dict]]  # [{bucket_start, bucket_end, count}]


@dataclass
class CategoricalStats:
    """Statistics for categorical features."""
    count: int
    missing: int
    missing_pct: float
    unique: int
    top_values: List[Dict]  # [{value, frequency, count}]
    avg_length: Optional[float]


@dataclass
class FeatureStatistics:
    """Complete statistics for a single feature."""
    name: str
    feature_type: str  # 'INT', 'FLOAT', 'STRING', 'BYTES', 'UNKNOWN'
    stats_type: str  # 'numeric' or 'categorical'
    numeric_stats: Optional[NumericStats]
    categorical_stats: Optional[CategoricalStats]


class StatisticsParser:
    """
    Parser for TFDV statistics files.

    Reads FeatureStats.pb from GCS and extracts comprehensive statistics
    matching the standard TFDV visualization format.
    """

    def __init__(self, project_id: str = None):
        self.project_id = project_id
        self._storage_client = None

    @property
    def storage_client(self):
        if self._storage_client is None:
            self._storage_client = storage.Client(project=self.project_id)
        return self._storage_client

    def parse_from_gcs(self, gcs_path: str) -> Dict:
        """
        Parse statistics from a GCS path.

        Args:
            gcs_path: Full GCS path to FeatureStats.pb or pipeline_root

        Returns:
            Dict with parsed statistics
        """
        try:
            # If path is pipeline_root, find the statistics file
            if not gcs_path.endswith('.pb'):
                stats_path = self._find_statistics_file(gcs_path)
                if not stats_path:
                    return {
                        'available': False,
                        'error': 'Statistics file not found in pipeline artifacts'
                    }
            else:
                stats_path = gcs_path

            # Download and parse the proto
            stats = self._load_statistics_proto(stats_path)
            if not stats:
                return {
                    'available': False,
                    'error': 'Failed to parse statistics file'
                }

            # Extract comprehensive statistics
            return self._extract_statistics(stats)

        except Exception as e:
            logger.exception(f"Error parsing statistics: {e}")
            return {
                'available': False,
                'error': str(e)
            }

    def _find_statistics_file(self, pipeline_root: str) -> Optional[str]:
        """Find FeatureStats.pb file in pipeline artifacts."""
        # Parse bucket and prefix
        path = pipeline_root.replace('gs://', '')
        parts = path.split('/', 1)
        bucket_name = parts[0]
        prefix = parts[1] if len(parts) > 1 else ''

        logger.info(f"Searching for statistics in gs://{bucket_name}/{prefix}")

        try:
            bucket = self.storage_client.bucket(bucket_name)
            blobs = list(bucket.list_blobs(prefix=prefix, max_results=500))
            logger.info(f"Found {len(blobs)} blobs in pipeline artifacts")
        except Exception as e:
            logger.error(f"Error listing blobs: {e}")
            return None

        # Look for StatisticsGen output (train split preferred)
        for blob in blobs:
            if ('StatisticsGen' in blob.name and
                'statistics' in blob.name and
                'Split-train' in blob.name and
                blob.name.endswith('FeatureStats.pb')):
                logger.info(f"Found statistics file: {blob.name}")
                return f"gs://{bucket_name}/{blob.name}"

        # Fallback: any FeatureStats.pb
        for blob in blobs:
            if blob.name.endswith('FeatureStats.pb'):
                logger.info(f"Found fallback statistics file: {blob.name}")
                return f"gs://{bucket_name}/{blob.name}"

        logger.warning(f"No FeatureStats.pb found in {len(blobs)} blobs")
        return None

    def _load_statistics_proto(self, gcs_path: str) -> Optional[statistics_pb2.DatasetFeatureStatisticsList]:
        """Load and parse statistics proto from GCS."""
        path = gcs_path.replace('gs://', '')
        parts = path.split('/', 1)
        bucket_name = parts[0]
        blob_name = parts[1]

        bucket = self.storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        if not blob.exists():
            logger.warning(f"Statistics blob not found: {gcs_path}")
            return None

        content = blob.download_as_bytes()
        stats = statistics_pb2.DatasetFeatureStatisticsList()
        stats.ParseFromString(content)

        logger.info(f"Loaded statistics with {len(stats.datasets)} datasets")
        return stats

    def _extract_statistics(self, stats: statistics_pb2.DatasetFeatureStatisticsList) -> Dict:
        """Extract comprehensive statistics from proto."""
        if not stats.datasets:
            return {
                'available': False,
                'error': 'No datasets in statistics file'
            }

        # Use first dataset (usually 'train')
        dataset = stats.datasets[0]
        dataset_name = dataset.name or 'train'
        num_examples = dataset.num_examples

        numeric_features = []
        categorical_features = []

        for feature in dataset.features:
            feature_stats = self._parse_feature(feature, num_examples)
            if feature_stats:
                if feature_stats.stats_type == 'numeric':
                    numeric_features.append(asdict(feature_stats))
                else:
                    categorical_features.append(asdict(feature_stats))

        # Calculate summary statistics
        all_features = numeric_features + categorical_features
        total_missing = 0
        for f in all_features:
            ns = f.get('numeric_stats') or {}
            cs = f.get('categorical_stats') or {}
            total_missing += ns.get('missing_pct', 0) or cs.get('missing_pct', 0) or 0
        avg_missing = total_missing / len(all_features) if all_features else 0

        return {
            'available': True,
            'dataset_name': dataset_name,
            'num_examples': num_examples,
            'num_features': len(all_features),
            'num_numeric_features': len(numeric_features),
            'num_categorical_features': len(categorical_features),
            'avg_missing_pct': round(avg_missing, 2),
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
        }

    def _parse_feature(self, feature, num_examples: int) -> Optional[FeatureStatistics]:
        """Parse statistics for a single feature."""
        # Get feature name
        try:
            if feature.path and feature.path.step:
                name = feature.path.step[0]
            else:
                name = getattr(feature, 'name', 'unknown')
        except (AttributeError, IndexError):
            name = 'unknown'

        # Determine feature type and parse accordingly
        try:
            if feature.HasField('num_stats'):
                return self._parse_numeric_feature(name, feature, num_examples)
            elif feature.HasField('string_stats'):
                return self._parse_categorical_feature(name, feature, num_examples)
            elif feature.HasField('bytes_stats'):
                return self._parse_bytes_feature(name, feature, num_examples)
        except (ValueError, AttributeError):
            pass

        # Unknown type
        return FeatureStatistics(
            name=name,
            feature_type='UNKNOWN',
            stats_type='categorical',
            numeric_stats=None,
            categorical_stats=CategoricalStats(
                count=num_examples,
                missing=0,
                missing_pct=0,
                unique=0,
                top_values=[],
                avg_length=None
            )
        )

    def _parse_numeric_feature(self, name: str, feature, num_examples: int) -> FeatureStatistics:
        """Parse numeric feature statistics."""
        num_stats = feature.num_stats

        # Calculate counts
        num_missing = getattr(feature, 'num_missing', 0) or 0
        count = num_examples - num_missing
        missing_pct = (num_missing / num_examples * 100) if num_examples > 0 else 0

        # Get basic statistics
        mean = num_stats.mean if hasattr(num_stats, 'mean') else None
        std_dev = num_stats.std_dev if hasattr(num_stats, 'std_dev') else None
        min_val = num_stats.min if hasattr(num_stats, 'min') and num_stats.min != float('inf') else None
        max_val = num_stats.max if hasattr(num_stats, 'max') and num_stats.max != float('-inf') else None
        median = num_stats.median if hasattr(num_stats, 'median') else None

        # Get zeros count
        zeros = num_stats.num_zeros if hasattr(num_stats, 'num_zeros') else 0
        zeros_pct = (zeros / count * 100) if count > 0 else 0

        # Extract histogram data
        histogram = self._extract_histogram(num_stats)

        # Determine if INT or FLOAT based on values
        feature_type = 'INT'
        if mean is not None and not float(mean).is_integer():
            feature_type = 'FLOAT'
        if std_dev is not None and not float(std_dev).is_integer():
            feature_type = 'FLOAT'

        return FeatureStatistics(
            name=name,
            feature_type=feature_type,
            stats_type='numeric',
            numeric_stats=NumericStats(
                count=count,
                missing=num_missing,
                missing_pct=round(missing_pct, 2),
                mean=self._safe_round(mean),
                std_dev=self._safe_round(std_dev),
                zeros=zeros,
                zeros_pct=round(zeros_pct, 2),
                min_val=self._safe_round(min_val),
                median=self._safe_round(median),
                max_val=self._safe_round(max_val),
                histogram=histogram
            ),
            categorical_stats=None
        )

    def _parse_categorical_feature(self, name: str, feature, num_examples: int) -> FeatureStatistics:
        """Parse categorical (string) feature statistics."""
        string_stats = feature.string_stats

        # Calculate counts
        num_missing = getattr(feature, 'num_missing', 0) or 0
        count = num_examples - num_missing
        missing_pct = (num_missing / num_examples * 100) if num_examples > 0 else 0

        # Get unique count
        unique = string_stats.unique if hasattr(string_stats, 'unique') else 0

        # Get average length
        avg_length = string_stats.avg_length if hasattr(string_stats, 'avg_length') else None

        # Extract top values
        top_values = []
        if hasattr(string_stats, 'top_values'):
            for tv in string_stats.top_values[:10]:  # Limit to top 10
                freq = tv.frequency if hasattr(tv, 'frequency') else 0
                freq_pct = (freq / count * 100) if count > 0 else 0
                top_values.append({
                    'value': tv.value if hasattr(tv, 'value') else str(tv),
                    'frequency': freq,
                    'frequency_pct': round(freq_pct, 2)
                })

        # Also check rank_histogram for more detailed distribution
        if hasattr(string_stats, 'rank_histogram') and string_stats.rank_histogram.buckets:
            if not top_values:  # Only use if top_values empty
                for bucket in string_stats.rank_histogram.buckets[:10]:
                    freq_pct = (bucket.sample_count / count * 100) if count > 0 else 0
                    top_values.append({
                        'value': bucket.label,
                        'frequency': bucket.sample_count,
                        'frequency_pct': round(freq_pct, 2)
                    })

        return FeatureStatistics(
            name=name,
            feature_type='STRING',
            stats_type='categorical',
            numeric_stats=None,
            categorical_stats=CategoricalStats(
                count=count,
                missing=num_missing,
                missing_pct=round(missing_pct, 2),
                unique=unique,
                top_values=top_values,
                avg_length=self._safe_round(avg_length)
            )
        )

    def _parse_bytes_feature(self, name: str, feature, num_examples: int) -> FeatureStatistics:
        """Parse bytes feature statistics (treated as categorical)."""
        bytes_stats = feature.bytes_stats

        num_missing = getattr(feature, 'num_missing', 0) or 0
        count = num_examples - num_missing
        missing_pct = (num_missing / num_examples * 100) if num_examples > 0 else 0

        unique = bytes_stats.unique if hasattr(bytes_stats, 'unique') else 0
        avg_length = bytes_stats.avg_num_bytes if hasattr(bytes_stats, 'avg_num_bytes') else None

        return FeatureStatistics(
            name=name,
            feature_type='BYTES',
            stats_type='categorical',
            numeric_stats=None,
            categorical_stats=CategoricalStats(
                count=count,
                missing=num_missing,
                missing_pct=round(missing_pct, 2),
                unique=unique,
                top_values=[],
                avg_length=self._safe_round(avg_length)
            )
        )

    def _extract_histogram(self, num_stats) -> Optional[List[Dict]]:
        """Extract histogram data from numeric statistics."""
        histogram = []

        # Try standard histogram first
        if hasattr(num_stats, 'histograms') and num_stats.histograms:
            hist = num_stats.histograms[0]  # Use first histogram
            if hasattr(hist, 'buckets'):
                for bucket in hist.buckets:
                    histogram.append({
                        'low': self._safe_round(bucket.low_value) if hasattr(bucket, 'low_value') else None,
                        'high': self._safe_round(bucket.high_value) if hasattr(bucket, 'high_value') else None,
                        'count': bucket.sample_count if hasattr(bucket, 'sample_count') else 0
                    })

        # Fallback: try common_stats histogram
        if not histogram:
            try:
                if hasattr(num_stats, 'common_stats') and num_stats.common_stats:
                    cs = num_stats.common_stats
                    if hasattr(cs, 'num_values_histogram') and cs.num_values_histogram.buckets:
                        for bucket in cs.num_values_histogram.buckets:
                            histogram.append({
                                'low': self._safe_round(bucket.low_value),
                                'high': self._safe_round(bucket.high_value),
                                'count': bucket.sample_count
                            })
            except (AttributeError, TypeError):
                pass

        return histogram if histogram else None

    def _safe_round(self, value, decimals: int = 4) -> Optional[float]:
        """Safely round a value, handling None and special floats."""
        if value is None:
            return None
        try:
            if math.isnan(value) or math.isinf(value):
                return None
            return round(float(value), decimals)
        except (TypeError, ValueError):
            return None

    def generate_html_visualization(self, gcs_path: str) -> Optional[str]:
        """
        Generate TFDV HTML visualization.

        Args:
            gcs_path: Path to statistics file or pipeline_root

        Returns:
            HTML string with TFDV visualization
        """
        try:
            # Find and load statistics
            if not gcs_path.endswith('.pb'):
                stats_path = self._find_statistics_file(gcs_path)
                if not stats_path:
                    return None
            else:
                stats_path = gcs_path

            # Load using TFDV for visualization
            stats = self._load_statistics_proto(stats_path)
            if not stats:
                return None

            # Generate HTML using TFDV
            html = tfdv.utils.display_util.get_statistics_html(stats)
            return html

        except Exception as e:
            logger.exception(f"Error generating HTML visualization: {e}")
            return None
